#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºç‰¹å¾å­¦ä¹ çš„è®¤çŸ¥è¡°å¼±æ•°æ®å¢å¼ºç³»ç»Ÿ
æ–¹æ¡ˆï¼šåˆ†ç±»åˆ«å­¦ä¹ ç‰¹å¾ -> æ•°æ®æ‰°åŠ¨ -> VAE-GANå¢å¼º -> è´¨é‡è¯„ä¼°
"""

import numpy as np
import pandas as pd
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report
from scipy import stats
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®matplotlibé¿å…ä¸­æ–‡å­—ä½“è­¦å‘Š
plt.rcParams['font.family'] = ['DejaVu Sans', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# å¯¼å…¥å¯è§†åŒ–å·¥å…·
try:
    from visualization_utils import create_comprehensive_report
except ImportError:
    print("âš ï¸ å¯è§†åŒ–æ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†è·³è¿‡å¯è§†åŒ–åŠŸèƒ½")
    create_comprehensive_report = None

class FeatureLearner:
    """ç±»åˆ«ç‰¹å¾å­¦ä¹ å™¨"""
    
    def __init__(self, n_components=10):
        self.n_components = n_components
        self.class_features = {}
        self.class_scalers = {}
        self.class_pcas = {}
        
    def learn_class_features(self, data_by_class):
        """å­¦ä¹ æ¯ä¸ªç±»åˆ«çš„ç‰¹å¾åˆ†å¸ƒ"""
        print("ğŸ§  å­¦ä¹ å„ç±»åˆ«ç‰¹å¾åˆ†å¸ƒ...")
        
        for class_label, class_data in data_by_class.items():
            if len(class_data) == 0:
                continue
                
            print(f"   ğŸ“Š å­¦ä¹ ç±»åˆ« {class_label} ç‰¹å¾ ({len(class_data)} ä¸ªæ ·æœ¬)")
            
            # å±•å¹³æ•°æ®
            flattened_data = np.array([sample.flatten() for sample in class_data])
            
            # æ ‡å‡†åŒ–
            scaler = StandardScaler()
            scaled_data = scaler.fit_transform(flattened_data)
            
            # PCAé™ç»´æå–ä¸»è¦ç‰¹å¾
            n_comp = min(self.n_components, len(class_data)-1, scaled_data.shape[1])
            pca = PCA(n_components=n_comp)
            pca_features = pca.fit_transform(scaled_data)
            
            # è®¡ç®—ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
            feature_stats = {
                'mean': np.mean(pca_features, axis=0),
                'std': np.std(pca_features, axis=0),
                'min': np.min(pca_features, axis=0),
                'max': np.max(pca_features, axis=0),
                'median': np.median(pca_features, axis=0)
            }
            
            # ä½¿ç”¨K-meansèšç±»å‘ç°å­æ¨¡å¼
            if len(class_data) >= 3:
                n_clusters = min(3, len(class_data))
                kmeans = KMeans(n_clusters=n_clusters, random_state=42)
                clusters = kmeans.fit_predict(pca_features)
                cluster_centers = kmeans.cluster_centers_
            else:
                clusters = np.zeros(len(class_data))
                cluster_centers = np.array([feature_stats['mean']])
            
            self.class_features[class_label] = {
                'stats': feature_stats,
                'clusters': clusters,
                'cluster_centers': cluster_centers,
                'raw_features': pca_features
            }
            self.class_scalers[class_label] = scaler
            self.class_pcas[class_label] = pca
            
            print(f"     âœ… æå– {n_comp} ä¸ªä¸»è¦ç‰¹å¾")
    
    def generate_perturbation_patterns(self, class_label, n_patterns=5):
        """ä¸ºæŒ‡å®šç±»åˆ«ç”Ÿæˆæ‰°åŠ¨æ¨¡å¼"""
        if class_label not in self.class_features:
            return []
        
        features = self.class_features[class_label]
        stats = features['stats']
        
        patterns = []
        
        # æ¨¡å¼1: åŸºäºå‡å€¼å’Œæ ‡å‡†å·®çš„é«˜æ–¯æ‰°åŠ¨
        for i in range(n_patterns):
            noise_scale = 0.1 + i * 0.05  # é€’å¢çš„å™ªå£°å¼ºåº¦
            pattern = {
                'type': 'gaussian',
                'mean': stats['mean'],
                'std': stats['std'] * noise_scale,
                'intensity': noise_scale
            }
            patterns.append(pattern)
        
        # æ¨¡å¼2: åŸºäºèšç±»ä¸­å¿ƒçš„æ‰°åŠ¨
        for center in features['cluster_centers']:
            pattern = {
                'type': 'cluster_based',
                'center': center,
                'std': stats['std'] * 0.15,
                'intensity': 0.15
            }
            patterns.append(pattern)
        
        return patterns

class SimpleVAEGAN(nn.Module):
    """ç®€åŒ–çš„VAE-GANç½‘ç»œ"""
    
    def __init__(self, input_dim, latent_dim=32, hidden_dim=128):
        super(SimpleVAEGAN, self).__init__()
        self.input_dim = input_dim
        self.latent_dim = latent_dim
        
        # VAEç¼–ç å™¨
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU()
        )
        self.mu_layer = nn.Linear(hidden_dim//2, latent_dim)
        self.logvar_layer = nn.Linear(hidden_dim//2, latent_dim)
        
        # VAEè§£ç å™¨
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Linear(hidden_dim//2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            nn.Tanh()
        )
        
        # åˆ¤åˆ«å™¨
        self.discriminator = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim//2, 1),
            nn.Sigmoid()
        )
    
    def encode(self, x):
        h = self.encoder(x)
        mu = self.mu_layer(h)
        logvar = self.logvar_layer(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        return self.decoder(z)
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon = self.decode(z)
        return recon, mu, logvar
    
    def discriminate(self, x):
        return self.discriminator(x)

class FeatureBasedAugmentationSystem:
    """åŸºäºç‰¹å¾å­¦ä¹ çš„æ•°æ®å¢å¼ºç³»ç»Ÿ"""
    
    def __init__(self, seq_len=400, latent_dim=32, device='cuda'):
        self.seq_len = seq_len
        self.latent_dim = latent_dim
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
        self.feature_learner = FeatureLearner()
        self.vae_gan = None
        self.global_scaler = MinMaxScaler(feature_range=(0.01, 0.99))
        
        print(f"ğŸ”§ åˆå§‹åŒ–å¢å¼ºç³»ç»Ÿï¼Œè®¾å¤‡: {self.device}")
    
    def load_and_process_data(self, data_dir):
        """åŠ è½½å’Œå¤„ç†åŸå§‹æ•°æ®"""
        print(f"ğŸ“‚ ä» {data_dir} åŠ è½½æ•°æ®...")
        
        if not os.path.exists(data_dir):
            raise ValueError(f"ç›®å½• '{data_dir}' ä¸å­˜åœ¨")
        
        data_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]
        if not data_files:
            raise ValueError(f"åœ¨ {data_dir} ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")
        
        all_data = []
        all_labels = []
        data_by_class = {0: [], 1: [], 2: []}
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶")
        
        for file in data_files:
            try:
                df = pd.read_csv(os.path.join(data_dir, file))
                
                # æå–æ ‡ç­¾
                if 'label' in df.columns:
                    label = int(df['label'].iloc[0])
                    df = df.drop('label', axis=1)
                else:
                    # ä»æ–‡ä»¶åæå–æ ‡ç­¾
                    import re
                    numbers = re.findall(r'\d+', file)
                    label = int(numbers[0]) % 3 if numbers else 0
                
                # æ•°æ®é¢„å¤„ç†
                data = df.values.astype(np.float32)
                data = np.nan_to_num(data, nan=0.0, posinf=1.0, neginf=-1.0)
                
                # æˆªæ–­æˆ–å¡«å……åˆ°æŒ‡å®šé•¿åº¦
                if len(data) > self.seq_len:
                    data = data[:self.seq_len]
                else:
                    padding = np.zeros((self.seq_len - len(data), data.shape[1]))
                    data = np.vstack([data, padding])
                
                all_data.append(data)
                all_labels.append(label)
                
                # æŒ‰ç±»åˆ«åˆ†ç»„
                if label in data_by_class:
                    data_by_class[label].append(data)
                
            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}")
                continue
        
        if not all_data:
            raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®æ–‡ä»¶")
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
        print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {len(all_data)}")
        print(f"ğŸ“Š æ¯ä¸ªæ ·æœ¬å½¢çŠ¶: {all_data[0].shape}")
        print(f"ğŸ·ï¸ æ ‡ç­¾åˆ†å¸ƒ: {[len(data_by_class[i]) for i in range(3)]}")
        
        return all_data, np.array(all_labels), data_by_class
    
    def apply_feature_based_perturbation(self, data, class_label, perturbation_pattern):
        """åº”ç”¨åŸºäºç‰¹å¾å­¦ä¹ çš„æ•°æ®æ‰°åŠ¨"""
        if class_label not in self.feature_learner.class_scalers:
            return data
        
        # è·å–å¯¹åº”çš„scalerå’Œpca
        scaler = self.feature_learner.class_scalers[class_label]
        pca = self.feature_learner.class_pcas[class_label]
        
        # å±•å¹³å¹¶æ ‡å‡†åŒ–
        flat_data = data.flatten().reshape(1, -1)
        scaled_data = scaler.transform(flat_data)
        
        # PCAå˜æ¢åˆ°ç‰¹å¾ç©ºé—´
        pca_data = pca.transform(scaled_data)
        
        # åº”ç”¨æ‰°åŠ¨
        if perturbation_pattern['type'] == 'gaussian':
            noise = np.random.normal(0, perturbation_pattern['std'], pca_data.shape)
            perturbed_pca = pca_data + noise
        elif perturbation_pattern['type'] == 'cluster_based':
            direction = perturbation_pattern['center'] - pca_data[0]
            noise = np.random.normal(0, perturbation_pattern['std'], pca_data.shape)
            perturbed_pca = pca_data + 0.3 * direction + noise
        else:
            perturbed_pca = pca_data
        
        # é€†å˜æ¢å›åŸå§‹ç©ºé—´
        perturbed_scaled = pca.inverse_transform(perturbed_pca)
        perturbed_flat = scaler.inverse_transform(perturbed_scaled)
        
        # é‡å¡‘å›åŸå§‹å½¢çŠ¶
        perturbed_data = perturbed_flat.reshape(data.shape)
        
        return perturbed_data

    def train_vae_gan(self, data_list, labels, epochs=100, batch_size=16):
        """è®­ç»ƒæ”¹è¿›çš„VAE-GANç½‘ç»œ"""
        print("ğŸ”§ è®­ç»ƒæ”¹è¿›çš„VAE-GANç½‘ç»œ...")

        # å‡†å¤‡è®­ç»ƒæ•°æ®
        flattened_data = np.array([data.flatten() for data in data_list])

        # å…¨å±€æ ‡å‡†åŒ–
        scaled_data = self.global_scaler.fit_transform(flattened_data)

        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        tensor_data = torch.FloatTensor(scaled_data).to(self.device)
        tensor_labels = torch.LongTensor(labels).to(self.device)

        dataset = TensorDataset(tensor_data, tensor_labels)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)

        # åˆå§‹åŒ–VAE-GAN
        input_dim = scaled_data.shape[1]
        self.vae_gan = SimpleVAEGAN(input_dim, self.latent_dim).to(self.device)

        # æ”¹è¿›çš„ä¼˜åŒ–å™¨è®¾ç½® - ä¸åŒå­¦ä¹ ç‡
        optimizer_vae = optim.Adam(
            list(self.vae_gan.encoder.parameters()) + list(self.vae_gan.decoder.parameters()),
            lr=2e-4, betas=(0.5, 0.999)  # ç”Ÿæˆå™¨å­¦ä¹ ç‡ç¨é«˜
        )
        optimizer_disc = optim.Adam(
            self.vae_gan.discriminator.parameters(),
            lr=1e-4, betas=(0.5, 0.999)  # åˆ¤åˆ«å™¨å­¦ä¹ ç‡è¾ƒä½
        )

        # æŸå¤±å‡½æ•°
        mse_loss = nn.MSELoss()
        bce_loss = nn.BCELoss()

        print(f"   ğŸ“Š è®­ç»ƒæ•°æ®å½¢çŠ¶: {scaled_data.shape}")
        print(f"   ğŸ”§ ç½‘ç»œè¾“å…¥ç»´åº¦: {input_dim}")

        # è®­ç»ƒå†å²è®°å½•å’Œæ—©åœæœºåˆ¶
        vae_losses = []
        disc_losses = []
        best_vae_loss = float('inf')
        patience = 50
        patience_counter = 0

        for epoch in range(epochs):
            epoch_vae_loss = 0
            epoch_disc_loss = 0

            for batch_idx, (batch_data, batch_labels) in enumerate(dataloader):
                batch_size_actual = batch_data.size(0)

                # åŠ¨æ€è°ƒæ•´è®­ç»ƒé¢‘ç‡ - é˜²æ­¢åˆ¤åˆ«å™¨è¿‡å¼º
                train_disc = (batch_idx % 2 == 0) or (epoch < epochs // 4)

                # è®­ç»ƒVAE (æ¯ä¸ªbatchéƒ½è®­ç»ƒ)
                optimizer_vae.zero_grad()

                recon_data, mu, logvar = self.vae_gan(batch_data)

                # VAEæŸå¤± - è°ƒæ•´æƒé‡
                recon_loss = mse_loss(recon_data, batch_data)
                kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
                kl_loss /= batch_size_actual * input_dim

                # å¯¹æŠ—æŸå¤± - å¢åŠ æƒé‡
                fake_validity = self.vae_gan.discriminate(recon_data)
                adv_loss = bce_loss(fake_validity, torch.ones_like(fake_validity))

                # è°ƒæ•´æŸå¤±æƒé‡
                beta_kl = min(1.0, epoch / (epochs * 0.5))  # KLæƒé‡é€æ¸å¢åŠ 
                alpha_adv = 0.1 if epoch < epochs // 2 else 0.05  # å¯¹æŠ—æƒé‡åæœŸé™ä½

                vae_loss = recon_loss + beta_kl * 0.1 * kl_loss + alpha_adv * adv_loss
                vae_loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(
                    list(self.vae_gan.encoder.parameters()) + list(self.vae_gan.decoder.parameters()),
                    max_norm=1.0
                )

                optimizer_vae.step()

                # è®­ç»ƒåˆ¤åˆ«å™¨ (é™ä½é¢‘ç‡)
                if train_disc:
                    optimizer_disc.zero_grad()

                    real_validity = self.vae_gan.discriminate(batch_data)
                    fake_validity = self.vae_gan.discriminate(recon_data.detach())

                    # æ ‡ç­¾å¹³æ»‘
                    real_labels = torch.ones_like(real_validity) * 0.9
                    fake_labels = torch.zeros_like(fake_validity) + 0.1

                    real_loss = bce_loss(real_validity, real_labels)
                    fake_loss = bce_loss(fake_validity, fake_labels)
                    disc_loss = (real_loss + fake_loss) / 2

                    disc_loss.backward()

                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(
                        self.vae_gan.discriminator.parameters(),
                        max_norm=1.0
                    )

                    optimizer_disc.step()
                    epoch_disc_loss += disc_loss.item()

                epoch_vae_loss += vae_loss.item()

            # è®°å½•æŸå¤±
            vae_losses.append(epoch_vae_loss / len(dataloader))
            disc_losses.append(epoch_disc_loss / len(dataloader) if epoch_disc_loss > 0 else 0)

            # æ—©åœæ£€æŸ¥
            current_vae_loss = vae_losses[-1]
            if current_vae_loss < best_vae_loss:
                best_vae_loss = current_vae_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€
                best_model_state = {
                    'encoder': self.vae_gan.encoder.state_dict(),
                    'decoder': self.vae_gan.decoder.state_dict(),
                    'discriminator': self.vae_gan.discriminator.state_dict()
                }
            else:
                patience_counter += 1

            # æ‰“å°è¿›åº¦å’ŒæŸå¤±è¶‹åŠ¿åˆ†æ
            if (epoch + 1) % 50 == 0:
                recent_vae = np.mean(vae_losses[-10:])
                recent_disc = np.mean(disc_losses[-10:])

                print(f"   Epoch {epoch+1}/{epochs}:")
                print(f"     VAE Loss: {recent_vae:.4f}, Disc Loss: {recent_disc:.4f}")
                print(f"     Best VAE Loss: {best_vae_loss:.4f}, Patience: {patience_counter}/{patience}")

                # è®­ç»ƒç¨³å®šæ€§æ£€æŸ¥
                if len(vae_losses) >= 20:
                    vae_trend = np.polyfit(range(10), vae_losses[-10:], 1)[0]
                    disc_trend = np.polyfit(range(10), disc_losses[-10:], 1)[0]

                    if vae_trend > 0.01:
                        print(f"     âš ï¸ VAEæŸå¤±ä¸Šå‡è¶‹åŠ¿: {vae_trend:.6f}")
                    if disc_trend > 0.01:
                        print(f"     âš ï¸ åˆ¤åˆ«å™¨æŸå¤±ä¸Šå‡è¶‹åŠ¿: {disc_trend:.6f}")

                    # è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´
                    if disc_trend > 0.02:  # åˆ¤åˆ«å™¨æŸå¤±ä¸Šå‡è¿‡å¿«
                        for param_group in optimizer_disc.param_groups:
                            param_group['lr'] *= 0.9
                        print(f"     ğŸ”§ é™ä½åˆ¤åˆ«å™¨å­¦ä¹ ç‡è‡³: {optimizer_disc.param_groups[0]['lr']:.6f}")

            # æ—©åœæ¡ä»¶
            if patience_counter >= patience:
                print(f"   ğŸ›‘ æ—©åœè§¦å‘ï¼åœ¨epoch {epoch+1}åœæ­¢è®­ç»ƒ")
                # æ¢å¤æœ€ä½³æ¨¡å‹
                if 'best_model_state' in locals():
                    self.vae_gan.encoder.load_state_dict(best_model_state['encoder'])
                    self.vae_gan.decoder.load_state_dict(best_model_state['decoder'])
                    self.vae_gan.discriminator.load_state_dict(best_model_state['discriminator'])
                    print(f"   âœ… æ¢å¤æœ€ä½³æ¨¡å‹çŠ¶æ€ (VAE Loss: {best_vae_loss:.4f})")
                break

        print("âœ… VAE-GANè®­ç»ƒå®Œæˆ")
        print(f"   ğŸ“Š æœ€ç»ˆVAEæŸå¤±: {vae_losses[-1]:.4f}")
        print(f"   ğŸ“Š æœ€ç»ˆåˆ¤åˆ«å™¨æŸå¤±: {disc_losses[-1]:.4f}")

        return vae_losses, disc_losses

    def generate_augmented_samples(self, original_data, original_labels, n_augment_per_sample=2):
        """ç”Ÿæˆæ”¹è¿›çš„å¢å¼ºæ ·æœ¬"""
        print("ğŸ”„ ç”Ÿæˆæ”¹è¿›çš„å¢å¼ºæ ·æœ¬...")

        if self.vae_gan is None:
            raise ValueError("VAE-GANæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train_vae_gan")

        augmented_data = []
        augmented_labels = []

        # è®¡ç®—æ¯ä¸ªç±»åˆ«çš„æ ·æœ¬æ•°é‡ï¼Œç”¨äºå¹³è¡¡å¢å¼º
        class_counts = np.bincount(original_labels)
        max_class_count = np.max(class_counts)

        for sample_idx, (data, label) in enumerate(zip(original_data, original_labels)):
            # è·å–è¯¥ç±»åˆ«çš„æ‰°åŠ¨æ¨¡å¼
            perturbation_patterns = self.feature_learner.generate_perturbation_patterns(label)

            # åŠ¨æ€è°ƒæ•´å¢å¼ºæ•°é‡ - å°‘æ•°ç±»åˆ«ç”Ÿæˆæ›´å¤šæ ·æœ¬
            class_ratio = class_counts[label] / max_class_count
            adaptive_n_augment = max(1, int(n_augment_per_sample / class_ratio))
            adaptive_n_augment = min(adaptive_n_augment, n_augment_per_sample * 2)  # é™åˆ¶æœ€å¤§æ•°é‡

            for j in range(adaptive_n_augment):
                # é€‰æ‹©å¢å¼ºç­–ç•¥
                augment_strategy = j % 3  # ä¸‰ç§ç­–ç•¥è½®æ¢

                if augment_strategy == 0:
                    # ç­–ç•¥1: ä»…ç‰¹å¾æ‰°åŠ¨ï¼ˆä¿æŒåŸå§‹æ•°æ®ç‰¹æ€§ï¼‰
                    if perturbation_patterns:
                        pattern = perturbation_patterns[j % len(perturbation_patterns)]
                        # å‡å°‘æ‰°åŠ¨å¼ºåº¦
                        pattern['intensity'] *= 0.5
                        enhanced_data = self.apply_feature_based_perturbation(data, label, pattern)
                    else:
                        enhanced_data = data + np.random.normal(0, 0.01, data.shape)

                elif augment_strategy == 1:
                    # ç­–ç•¥2: VAEé‡æ„ï¼ˆè½»å¾®å˜æ¢ï¼‰
                    with torch.no_grad():
                        flat_data = data.flatten().reshape(1, -1)
                        scaled_data = self.global_scaler.transform(flat_data)
                        tensor_data = torch.FloatTensor(scaled_data).to(self.device)

                        # ä»…ä½¿ç”¨VAEé‡æ„ï¼Œä¸æ·»åŠ é¢å¤–å™ªå£°
                        recon_data, _, _ = self.vae_gan(tensor_data)

                        # ä¸åŸå§‹æ•°æ®æ··åˆ
                        mix_ratio = 0.7  # 70%åŸå§‹æ•°æ®ï¼Œ30%é‡æ„æ•°æ®
                        mixed_data = mix_ratio * tensor_data + (1 - mix_ratio) * recon_data

                        enhanced_scaled = mixed_data.cpu().numpy()
                        enhanced_flat = self.global_scaler.inverse_transform(enhanced_scaled)
                        enhanced_data = enhanced_flat.reshape(data.shape)

                else:
                    # ç­–ç•¥3: ç‰¹å¾æ‰°åŠ¨ + è½»å¾®VAEå¢å¼º
                    if perturbation_patterns:
                        pattern = perturbation_patterns[j % len(perturbation_patterns)]
                        pattern['intensity'] *= 0.3  # æ›´å°çš„æ‰°åŠ¨
                        perturbed_data = self.apply_feature_based_perturbation(data, label, pattern)
                    else:
                        perturbed_data = data

                    with torch.no_grad():
                        flat_data = perturbed_data.flatten().reshape(1, -1)
                        scaled_data = self.global_scaler.transform(flat_data)
                        tensor_data = torch.FloatTensor(scaled_data).to(self.device)

                        # åœ¨æ½œåœ¨ç©ºé—´æ·»åŠ å¾ˆå°çš„å™ªå£°
                        mu, _ = self.vae_gan.encode(tensor_data)
                        noise = torch.randn_like(mu) * 0.05  # å‡å°‘å™ªå£°å¼ºåº¦
                        z = mu + noise
                        enhanced_tensor = self.vae_gan.decode(z)

                        enhanced_scaled = enhanced_tensor.cpu().numpy()
                        enhanced_flat = self.global_scaler.inverse_transform(enhanced_scaled)
                        enhanced_data = enhanced_flat.reshape(data.shape)

                # è´¨é‡æ£€æŸ¥ - è¿‡æ»¤å¼‚å¸¸æ ·æœ¬
                if self._is_valid_sample(enhanced_data, data):
                    augmented_data.append(enhanced_data)
                    augmented_labels.append(label)
                else:
                    # å¦‚æœç”Ÿæˆçš„æ ·æœ¬è´¨é‡ä¸å¥½ï¼Œä½¿ç”¨è½»å¾®å™ªå£°ç‰ˆæœ¬
                    fallback_data = data + np.random.normal(0, 0.005, data.shape)
                    augmented_data.append(fallback_data)
                    augmented_labels.append(label)

        print(f"âœ… ç”Ÿæˆ {len(augmented_data)} ä¸ªå¢å¼ºæ ·æœ¬")
        print(f"   ğŸ“Š å¹³å‡æ¯ä¸ªåŸå§‹æ ·æœ¬ç”Ÿæˆ {len(augmented_data)/len(original_data):.1f} ä¸ªå¢å¼ºæ ·æœ¬")
        return augmented_data, np.array(augmented_labels)

    def _is_valid_sample(self, enhanced_data, original_data, threshold=3.0):
        """æ£€æŸ¥ç”Ÿæˆæ ·æœ¬çš„æœ‰æ•ˆæ€§"""
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸å€¼
            if np.any(np.isnan(enhanced_data)) or np.any(np.isinf(enhanced_data)):
                return False

            # æ£€æŸ¥ä¸åŸå§‹æ•°æ®çš„å·®å¼‚æ˜¯å¦è¿‡å¤§
            diff_ratio = np.abs(enhanced_data - original_data) / (np.abs(original_data) + 1e-8)
            if np.mean(diff_ratio) > threshold:
                return False

            # æ£€æŸ¥æ–¹å·®æ˜¯å¦åˆç†
            if np.std(enhanced_data) < 0.001 or np.std(enhanced_data) > 100 * np.std(original_data):
                return False

            return True
        except:
            return False

    def compute_quality_score(self, sample, original_sample):
        """è®¡ç®—æ ·æœ¬è´¨é‡åˆ†æ•°"""
        score = 0.0

        # 1. ç»Ÿè®¡ç‰¹æ€§æ£€æŸ¥
        if not (np.isnan(sample).any() or np.isinf(sample).any()):
            score += 0.2

        # 2. æ–¹å·®æ£€æŸ¥
        if np.std(sample) > 0.01:
            score += 0.2

        # 3. èŒƒå›´åˆç†æ€§æ£€æŸ¥
        if np.abs(sample.mean()) < 10 * np.abs(original_sample.mean()):
            score += 0.2

        # 4. å½¢çŠ¶ç›¸ä¼¼æ€§æ£€æŸ¥
        try:
            correlation = np.corrcoef(sample.flatten(), original_sample.flatten())[0, 1]
            if not np.isnan(correlation) and correlation > 0.3:
                score += 0.2
        except:
            pass

        # 5. åˆ†å¸ƒç›¸ä¼¼æ€§æ£€æŸ¥
        try:
            _, p_value = stats.ks_2samp(sample.flatten(), original_sample.flatten())
            if p_value > 0.01:  # åˆ†å¸ƒä¸æ˜¾è‘—ä¸åŒ
                score += 0.2
        except:
            pass

        return score

    def evaluate_augmentation_quality(self, original_data, original_labels, augmented_data, augmented_labels):
        """è¯„ä¼°å¢å¼ºè´¨é‡"""
        print("ğŸ” è¯„ä¼°å¢å¼ºè´¨é‡...")

        # è®¡ç®—è´¨é‡åˆ†æ•°
        quality_scores = []
        for i, aug_sample in enumerate(augmented_data):
            orig_idx = i // 2  # å‡è®¾æ¯ä¸ªåŸå§‹æ ·æœ¬ç”Ÿæˆ2ä¸ªå¢å¼ºæ ·æœ¬
            if orig_idx < len(original_data):
                score = self.compute_quality_score(aug_sample, original_data[orig_idx])
                quality_scores.append(score)

        avg_quality = np.mean(quality_scores)
        print(f"   ğŸ“Š å¹³å‡è´¨é‡åˆ†æ•°: {avg_quality:.4f}")

        # åˆ†ç±»æ€§èƒ½è¯„ä¼°
        def flatten_data(data_list):
            return np.array([data.flatten() for data in data_list])

        X_orig = flatten_data(original_data)
        X_aug = flatten_data(augmented_data)

        # æ ‡å‡†åŒ–
        scaler = StandardScaler()
        X_orig_scaled = scaler.fit_transform(X_orig)
        X_aug_scaled = scaler.transform(X_aug)

        # PCAé™ç»´
        if X_orig_scaled.shape[1] > 100:
            n_components = min(20, X_orig_scaled.shape[0]-1, X_orig_scaled.shape[1])
            pca = PCA(n_components=n_components)
            X_orig_scaled = pca.fit_transform(X_orig_scaled)
            X_aug_scaled = pca.transform(X_aug_scaled)

        # åˆ†å‰²æ•°æ®
        if len(original_labels) >= 6:
            X_train, X_test, y_train, y_test = train_test_split(
                X_orig_scaled, original_labels, test_size=0.3, random_state=42,
                stratify=original_labels
            )

            # åŸºçº¿æµ‹è¯•
            clf_baseline = RandomForestClassifier(n_estimators=100, random_state=42)
            clf_baseline.fit(X_train, y_train)
            y_pred_baseline = clf_baseline.predict(X_test)
            acc_baseline = accuracy_score(y_test, y_pred_baseline)

            # å¢å¼ºæ•°æ®æµ‹è¯•
            X_combined = np.vstack([X_train, X_aug_scaled])
            y_combined = np.hstack([y_train, augmented_labels])

            clf_augmented = RandomForestClassifier(n_estimators=100, random_state=42)
            clf_augmented.fit(X_combined, y_combined)
            y_pred_augmented = clf_augmented.predict(X_test)
            acc_augmented = accuracy_score(y_test, y_pred_augmented)

            print(f"   ğŸ“Š åŸºçº¿å‡†ç¡®ç‡: {acc_baseline:.4f}")
            print(f"   ğŸ“Š å¢å¼ºå‡†ç¡®ç‡: {acc_augmented:.4f}")
            print(f"   ğŸ“ˆ æ€§èƒ½æå‡: {acc_augmented - acc_baseline:.4f}")

            return {
                'quality_score': avg_quality,
                'baseline_accuracy': acc_baseline,
                'augmented_accuracy': acc_augmented,
                'improvement': acc_augmented - acc_baseline
            }
        else:
            print("   âš ï¸ æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡åˆ†ç±»è¯„ä¼°")
            return {'quality_score': avg_quality}

    def save_augmented_data(self, augmented_data, augmented_labels, output_dir):
        """ä¿å­˜å¢å¼ºæ•°æ®ä¸ºå•ç‹¬çš„CSVæ–‡ä»¶"""
        print(f"ğŸ’¾ ä¿å­˜å¢å¼ºæ•°æ®åˆ° {output_dir}...")

        os.makedirs(output_dir, exist_ok=True)

        for i, (data, label) in enumerate(zip(augmented_data, augmented_labels)):
            # åˆ›å»ºDataFrame
            df = pd.DataFrame(data)
            df['label'] = label

            # ä¿å­˜ä¸ºCSVæ–‡ä»¶
            filename = f"augmented_data_{i:04d}.csv"
            filepath = os.path.join(output_dir, filename)
            df.to_csv(filepath, index=False)

        print(f"âœ… ä¿å­˜å®Œæˆï¼Œå…± {len(augmented_data)} ä¸ªæ–‡ä»¶")

        # ä¿å­˜è´¨é‡è¯„ä¼°æŠ¥å‘Š
        return output_dir

def run_feature_based_augmentation(config):
    """è¿è¡ŒåŸºäºç‰¹å¾å­¦ä¹ çš„æ•°æ®å¢å¼º"""

    print("ğŸ§  åŸºäºç‰¹å¾å­¦ä¹ çš„è®¤çŸ¥è¡°å¼±æ•°æ®å¢å¼ºç³»ç»Ÿ")
    print("=" * 60)

    # åˆå§‹åŒ–ç³»ç»Ÿ
    aug_system = FeatureBasedAugmentationSystem(
        seq_len=config['seq_len'],
        latent_dim=config['latent_dim'],
        device=config['device']
    )

    try:
        # 1. åŠ è½½å’Œå¤„ç†æ•°æ®
        original_data, original_labels, data_by_class = aug_system.load_and_process_data(
            config['data_path']
        )

        # 2. å­¦ä¹ å„ç±»åˆ«ç‰¹å¾
        aug_system.feature_learner.learn_class_features(data_by_class)

        # 3. è®­ç»ƒVAE-GANç½‘ç»œ
        aug_system.train_vae_gan(
            original_data,
            original_labels,
            epochs=config['vae_epochs'],
            batch_size=config['batch_size']
        )

        # 4. ç”Ÿæˆå¢å¼ºæ ·æœ¬
        augmented_data, augmented_labels = aug_system.generate_augmented_samples(
            original_data,
            original_labels,
            n_augment_per_sample=config['n_augment_per_sample']
        )

        # 5. è¯„ä¼°å¢å¼ºè´¨é‡
        quality_metrics = aug_system.evaluate_augmentation_quality(
            original_data, original_labels,
            augmented_data, augmented_labels
        )

        # 6. ä¿å­˜å¢å¼ºæ•°æ®
        output_dir = aug_system.save_augmented_data(
            augmented_data, augmented_labels, config['output_dir']
        )

        # 7. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
        if create_comprehensive_report is not None:
            create_comprehensive_report(
                original_data, augmented_data, original_labels, augmented_labels,
                quality_metrics, output_dir
            )

        # 8. ä¿å­˜è´¨é‡è¯„ä¼°æŠ¥å‘Š
        import json
        with open(os.path.join(output_dir, 'quality_report.json'), 'w', encoding='utf-8') as f:
            json.dump(quality_metrics, f, indent=2, ensure_ascii=False)

        # 9. æ‰“å°æ€»ç»“
        print("\n" + "=" * 60)
        print("ğŸ“Š å¢å¼ºå®Œæˆæ€»ç»“")
        print("=" * 60)
        print(f"ğŸ“‚ åŸå§‹æ•°æ®: {len(original_data)} ä¸ªæ ·æœ¬")
        print(f"ğŸ“‚ å¢å¼ºæ•°æ®: {len(augmented_data)} ä¸ªæ ·æœ¬")
        print(f"ğŸ“ˆ æ•°æ®å¢é•¿: {len(augmented_data) / len(original_data):.1f}x")
        print(f"ğŸ“Š è´¨é‡åˆ†æ•°: {quality_metrics.get('quality_score', 0):.4f}")

        if 'baseline_accuracy' in quality_metrics:
            print(f"ğŸ“Š åŸºçº¿å‡†ç¡®ç‡: {quality_metrics['baseline_accuracy']:.4f}")
            print(f"ğŸ“Š å¢å¼ºå‡†ç¡®ç‡: {quality_metrics['augmented_accuracy']:.4f}")
            print(f"ğŸ“ˆ æ€§èƒ½æå‡: {quality_metrics['improvement']:.4f}")

        print(f"ğŸ’¾ ç»“æœä¿å­˜åœ¨: {output_dir}")

        return quality_metrics

    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ä¸»å‡½æ•° - é…ç½®å‚æ•°å¹¶è¿è¡Œå¢å¼ºç³»ç»Ÿ"""

    # ==================== é…ç½®å‚æ•° ====================
    config = {
        # æ•°æ®è·¯å¾„é…ç½®
        'data_path': './feature1',  # åŸå§‹æ•°æ®ç›®å½•
        'output_dir': 'augmented_results',  # è¾“å‡ºç›®å½•

        # æ•°æ®å¤„ç†å‚æ•°
        'seq_len': 400,  # åºåˆ—é•¿åº¦

        # VAE-GANç½‘ç»œå‚æ•°
        'latent_dim': 64,  # æ½œåœ¨ç©ºé—´ç»´åº¦
        'vae_epochs': 1000,  # VAE-GANè®­ç»ƒè½®æ•°
        'batch_size': 8,  # æ‰¹æ¬¡å¤§å°
        'device': 'cuda',  # è®¡ç®—è®¾å¤‡

        # å¢å¼ºå‚æ•°
        'n_augment_per_sample': 2,  # æ¯ä¸ªåŸå§‹æ ·æœ¬ç”Ÿæˆçš„å¢å¼ºæ ·æœ¬æ•°

        # ç‰¹å¾å­¦ä¹ å‚æ•°
        'feature_components': 20,  # PCAç‰¹å¾æ•°é‡
    }
    # ================================================

    print("ğŸ“‹ é…ç½®ä¿¡æ¯:")
    for key, value in config.items():
        print(f"   {key}: {value}")
    print()

    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not os.path.exists(config['data_path']):
        print(f"âŒ é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨: {config['data_path']}")
        print("ğŸ’¡ è¯·ç¡®ä¿originalç›®å½•å­˜åœ¨å¹¶åŒ…å«CSVæ–‡ä»¶")
        return

    # è¿è¡Œå¢å¼ºç³»ç»Ÿ
    results = run_feature_based_augmentation(config)

    if results:
        print("\nğŸ‰ å¢å¼ºç³»ç»Ÿè¿è¡ŒæˆåŠŸ!")
    else:
        print("\nâŒ å¢å¼ºç³»ç»Ÿè¿è¡Œå¤±è´¥!")

if __name__ == "__main__":
    main()
