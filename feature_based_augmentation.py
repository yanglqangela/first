#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
åŸºäºç‰¹å¾å­¦ä¹ çš„è®¤çŸ¥è¡°å¼±æ•°æ®å¢å¼ºç³»ç»Ÿ
æ–¹æ¡ˆï¼šåˆ†ç±»åˆ«å­¦ä¹ ç‰¹å¾ -> æ•°æ®æ‰°åŠ¨ -> VAE-GANå¢å¼º -> è´¨é‡è¯„ä¼°
"""

import numpy as np
import pandas as pd
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, f1_score, classification_report
from scipy import stats
import matplotlib.pyplot as plt
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®matplotlibé¿å…ä¸­æ–‡å­—ä½“è­¦å‘Š
plt.rcParams['font.family'] = ['DejaVu Sans', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False

# å¯¼å…¥å¯è§†åŒ–å·¥å…·
try:
    from visualization_utils import create_comprehensive_report
except ImportError:
    print("âš ï¸ å¯è§†åŒ–æ¨¡å—æœªæ‰¾åˆ°ï¼Œå°†è·³è¿‡å¯è§†åŒ–åŠŸèƒ½")
    create_comprehensive_report = None

# ==================== åŸå‹ç½‘ç»œæ¨¡å— ====================
class ProtoNetEncoder(nn.Module):
    """è½»é‡åŒ–åŸå‹ç½‘ç»œç¼–ç å™¨ - ä¸“ä¸ºå°æ ·æœ¬ä¼˜åŒ–"""
    def __init__(self, input_dim, hidden_dim=64, z_dim=32, dropout_rate=0.2):
        super(ProtoNetEncoder, self).__init__()
        # ç®€åŒ–ç½‘ç»œç»“æ„ï¼Œå‡å°‘è¿‡æ‹Ÿåˆé£é™©
        self.net = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),
            nn.Linear(hidden_dim, z_dim),
            nn.BatchNorm1d(z_dim),
            nn.ReLU()
        )

        # æƒé‡åˆå§‹åŒ–
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                nn.init.constant_(m.bias, 0)

    def forward(self, x):
        return self.net(x)

class ProtoNet(nn.Module):
    """ä¼˜åŒ–çš„åŸå‹ç½‘ç»œåˆ†ç±»å™¨"""
    def __init__(self, encoder, temperature=1.0):
        super(ProtoNet, self).__init__()
        self.encoder = encoder
        self.temperature = temperature  # æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶softmaxçš„é”åº¦

    def compute_distances(self, query_embeddings, prototype_embeddings):
        """è®¡ç®—æŸ¥è¯¢æ ·æœ¬åˆ°åŸå‹çš„è·ç¦» - ä½¿ç”¨ä½™å¼¦ç›¸ä¼¼åº¦"""
        # L2å½’ä¸€åŒ–
        query_norm = torch.nn.functional.normalize(query_embeddings, p=2, dim=1)
        proto_norm = torch.nn.functional.normalize(prototype_embeddings, p=2, dim=1)

        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ (è½¬æ¢ä¸ºè·ç¦»)
        similarities = torch.mm(query_norm, proto_norm.t())
        distances = 1 - similarities  # ä½™å¼¦è·ç¦»

        return distances

    def forward(self, support_data, support_labels, query_data):
        """å‰å‘ä¼ æ’­ - ä¼˜åŒ–ç‰ˆæœ¬"""
        # ç¼–ç 
        support_embeddings = self.encoder(support_data)
        query_embeddings = self.encoder(query_data)

        # è®¡ç®—åŸå‹ - ä½¿ç”¨æ›´ç¨³å®šçš„æ–¹æ³•
        unique_labels = torch.unique(support_labels, sorted=True)
        n_class = len(unique_labels)
        z_dim = support_embeddings.size(1)

        prototypes = torch.zeros(n_class, z_dim, device=support_data.device)
        for i, label in enumerate(unique_labels):
            mask = support_labels == label
            class_embeddings = support_embeddings[mask]

            # ä¿®å¤å¼ é‡ç»´åº¦é—®é¢˜ - ä½¿ç”¨å‡å€¼è€Œéä¸­ä½æ•°
            if len(class_embeddings) > 1:
                prototypes[i] = torch.mean(class_embeddings, dim=0)
            else:
                prototypes[i] = class_embeddings[0]

        # è®¡ç®—è·ç¦»
        distances = self.compute_distances(query_embeddings, prototypes)
        return distances / self.temperature, unique_labels

class ProtoNetClassifier:
    """è‡ªé€‚åº”åŸå‹ç½‘ç»œåˆ†ç±»å™¨ - æ ¹æ®æ•°æ®é›†è‡ªåŠ¨è°ƒæ•´å‚æ•°"""
    def __init__(self, input_dim, hidden_dim=64, z_dim=32, dropout_rate=0.2,
                 learning_rate=0.01, epochs=15, device='cuda', temperature=2.0,
                 auto_adapt=True):
        self.input_dim = input_dim
        self.auto_adapt = auto_adapt

        # è‡ªåŠ¨é€‚åº”å‚æ•°
        if auto_adapt:
            hidden_dim, z_dim, dropout_rate, learning_rate, epochs, temperature = \
                self._auto_adapt_params(input_dim, hidden_dim, z_dim, dropout_rate,
                                      learning_rate, epochs, temperature)

        self.hidden_dim = hidden_dim
        self.z_dim = z_dim
        self.dropout_rate = dropout_rate
        self.learning_rate = learning_rate
        self.epochs = epochs
        self.device = device if torch.cuda.is_available() else 'cpu'
        self.temperature = temperature

        # åˆå§‹åŒ–ç½‘ç»œ
        encoder = ProtoNetEncoder(input_dim, hidden_dim, z_dim, dropout_rate)
        self.model = ProtoNet(encoder, temperature).to(self.device)

        # é€‰æ‹©ä¼˜åŒ–å™¨ - æ ¹æ®æ•°æ®è§„æ¨¡è‡ªåŠ¨é€‰æ‹©
        if input_dim < 50:  # å°ç‰¹å¾ç©ºé—´ä½¿ç”¨Adam
            self.optimizer = torch.optim.Adam(
                self.model.parameters(),
                lr=learning_rate,
                weight_decay=1e-4
            )
        else:  # å¤§ç‰¹å¾ç©ºé—´ä½¿ç”¨SGD
            self.optimizer = torch.optim.SGD(
                self.model.parameters(),
                lr=learning_rate,
                momentum=0.9,
                weight_decay=1e-4
            )

        self.criterion = nn.CrossEntropyLoss(label_smoothing=0.1)

        self.is_fitted = False
        self.support_data = None
        self.support_labels = None
        self.class_mapping = None

    def _auto_adapt_params(self, input_dim, hidden_dim, z_dim, dropout_rate,
                          learning_rate, epochs, temperature):
        """æ ¹æ®è¾“å…¥ç»´åº¦è‡ªåŠ¨è°ƒæ•´å‚æ•°"""

        # æ ¹æ®è¾“å…¥ç»´åº¦è°ƒæ•´ç½‘ç»œå¤§å°
        if input_dim <= 20:  # å°ç‰¹å¾ç©ºé—´ (å¦‚gaitæ•°æ®)
            hidden_dim = max(8, min(hidden_dim, input_dim * 2))
            z_dim = max(4, min(z_dim, input_dim))
            dropout_rate = min(0.5, dropout_rate + 0.1)  # å¢åŠ æ­£åˆ™åŒ–
            # ä¿æŒç”¨æˆ·é…ç½®çš„å­¦ä¹ ç‡ï¼Œä¸è‡ªåŠ¨è°ƒæ•´
            epochs = min(epochs, 15)                      # å‡å°‘è®­ç»ƒè½®æ•°
            temperature = max(2.0, temperature)           # å¢åŠ æ¸©åº¦

        elif input_dim <= 100:  # ä¸­ç­‰ç‰¹å¾ç©ºé—´
            hidden_dim = max(16, min(hidden_dim, input_dim))
            z_dim = max(8, min(z_dim, input_dim // 2))
            dropout_rate = max(0.2, dropout_rate)
            # ä¿æŒç”¨æˆ·é…ç½®çš„å­¦ä¹ ç‡
            epochs = min(epochs, 20)
            temperature = max(1.5, temperature)

        else:  # å¤§ç‰¹å¾ç©ºé—´ (å¦‚æ—¶åºæ•°æ®)
            hidden_dim = max(32, min(hidden_dim, input_dim // 4))
            z_dim = max(16, min(z_dim, input_dim // 8))
            dropout_rate = max(0.1, dropout_rate)
            # ä¿æŒç”¨æˆ·é…ç½®çš„å­¦ä¹ ç‡
            epochs = max(epochs, 20)
            temperature = max(1.0, temperature * 0.8)

        return hidden_dim, z_dim, dropout_rate, learning_rate, epochs, temperature

    def fit(self, X, y):
        """ä¼˜åŒ–çš„è®­ç»ƒæ–¹æ³• - é€‚åˆå°æ ·æœ¬"""
        self.model.train()

        # æ•°æ®é¢„å¤„ç†å’Œè½¬æ¢
        X_tensor = torch.FloatTensor(X).to(self.device)
        y_tensor = torch.LongTensor(y).to(self.device)

        # åˆ›å»ºç±»åˆ«æ˜ å°„
        unique_labels = torch.unique(y_tensor, sorted=True)
        self.class_mapping = {label.item(): i for i, label in enumerate(unique_labels)}

        # ä¿å­˜æ”¯æŒé›†
        self.support_data = X_tensor
        self.support_labels = y_tensor

        # å°æ‰¹é‡è®­ç»ƒ - ä¿®å¤æ‰¹æ¬¡å¤§å°é—®é¢˜
        n_classes = len(unique_labels)
        min_batch_size = max(4, n_classes * 2)  # ç¡®ä¿æ¯ä¸ªæ‰¹æ¬¡è‡³å°‘åŒ…å«æ¯ä¸ªç±»åˆ«2ä¸ªæ ·æœ¬
        batch_size = min(min_batch_size, len(X))



        dataset = TensorDataset(X_tensor, y_tensor)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        # å­¦ä¹ ç‡è°ƒåº¦å™¨ - ä¿®å¤å­¦ä¹ ç‡æ˜¾ç¤ºé—®é¢˜
        scheduler = torch.optim.lr_scheduler.StepLR(self.optimizer, step_size=max(3, self.epochs//3), gamma=0.9)

        best_loss = float('inf')
        patience = 5
        patience_counter = 0

        for epoch in range(self.epochs):
            epoch_loss = 0
            num_batches = 0

            for batch_x, batch_y in dataloader:
                if len(batch_x) < 2:  # è·³è¿‡å¤ªå°çš„æ‰¹æ¬¡
                    continue

                self.optimizer.zero_grad()

                # æ™ºèƒ½åˆ†å‰²ä¸ºæ”¯æŒé›†å’ŒæŸ¥è¯¢é›† - ç¡®ä¿ç±»åˆ«å¹³è¡¡
                unique_batch_labels = torch.unique(batch_y)

                if len(batch_x) >= len(unique_batch_labels) * 2:
                    # å¦‚æœæ ·æœ¬è¶³å¤Ÿï¼ŒæŒ‰ç±»åˆ«åˆ†å‰²
                    support_indices = []
                    query_indices = []

                    for label in unique_batch_labels:
                        label_indices = (batch_y == label).nonzero(as_tuple=True)[0]
                        if len(label_indices) >= 2:
                            # æ¯ä¸ªç±»åˆ«è‡³å°‘1ä¸ªæ”¯æŒæ ·æœ¬ï¼Œ1ä¸ªæŸ¥è¯¢æ ·æœ¬
                            support_indices.append(label_indices[0].item())
                            query_indices.extend([idx.item() for idx in label_indices[1:]])
                        else:
                            # å¦‚æœç±»åˆ«æ ·æœ¬ä¸è¶³ï¼ŒåŒæ—¶ç”¨ä½œæ”¯æŒå’ŒæŸ¥è¯¢
                            support_indices.extend([idx.item() for idx in label_indices])
                            query_indices.extend([idx.item() for idx in label_indices])

                    # è½¬æ¢ä¸ºå¼ é‡ç´¢å¼•
                    support_indices = torch.tensor(support_indices, device=batch_x.device)
                    query_indices = torch.tensor(query_indices, device=batch_x.device)

                    support_x = batch_x[support_indices]
                    support_y = batch_y[support_indices]
                    query_x = batch_x[query_indices]
                    query_y = batch_y[query_indices]
                else:
                    # å¦‚æœæ ·æœ¬ä¸è¶³ï¼Œä½¿ç”¨æ•´ä¸ªæ‰¹æ¬¡
                    support_x, support_y = batch_x, batch_y
                    query_x, query_y = batch_x, batch_y

                try:
                    distances, unique_labels = self.model(support_x, support_y, query_x)

                    # åˆ›å»ºæ ‡ç­¾æ˜ å°„
                    label_map = {label.item(): i for i, label in enumerate(unique_labels)}
                    mapped_labels = torch.tensor([label_map[label.item()] for label in query_y],
                                               device=self.device)

                    # è®¡ç®—æŸå¤±
                    logits = -distances
                    loss = self.criterion(logits, mapped_labels)

                    loss.backward()

                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                    self.optimizer.step()
                    epoch_loss += loss.item()
                    num_batches += 1

                except Exception:
                    # å¦‚æœå‡ºç°é”™è¯¯ï¼Œè·³è¿‡è¿™ä¸ªæ‰¹æ¬¡
                    continue

            if num_batches > 0:
                avg_loss = epoch_loss / num_batches
                scheduler.step()

                # æ—©åœæœºåˆ¶
                if avg_loss < best_loss:
                    best_loss = avg_loss
                    patience_counter = 0
                else:
                    patience_counter += 1

                if patience_counter >= patience:
                    print(f"   ProtoNet æ—©åœäº Epoch {epoch+1}, æœ€ä½³æŸå¤±: {best_loss:.4f}")
                    break

                if (epoch + 1) % 5 == 0:
                    current_lr = self.optimizer.param_groups[0]['lr']
                    print(f"   ProtoNet Epoch {epoch+1}/{self.epochs}, Loss: {avg_loss:.4f}, LR: {current_lr:.6f}")

        self.is_fitted = True
        return self

    def predict(self, X):
        """ä¼˜åŒ–çš„é¢„æµ‹æ–¹æ³•"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•")

        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(self.device)

            try:
                distances, unique_labels = self.model(self.support_data, self.support_labels, X_tensor)

                # é€‰æ‹©è·ç¦»æœ€å°çš„ç±»åˆ«
                predicted_indices = torch.argmin(distances, dim=1)
                predictions = unique_labels[predicted_indices].cpu().numpy()

                return predictions

            except Exception as e:
                # å¦‚æœé¢„æµ‹å¤±è´¥ï¼Œè¿”å›æœ€é¢‘ç¹çš„ç±»åˆ«
                most_common_label = torch.mode(self.support_labels)[0].item()
                return np.full(len(X), most_common_label)

    def predict_proba(self, X):
        """ä¼˜åŒ–çš„æ¦‚ç‡é¢„æµ‹"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨fitæ–¹æ³•")

        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(self.device)

            try:
                distances, unique_labels = self.model(self.support_data, self.support_labels, X_tensor)

                # å°†è·ç¦»è½¬æ¢ä¸ºæ¦‚ç‡ï¼ˆä½¿ç”¨æ¸©åº¦ç¼©æ”¾ï¼‰
                logits = -distances
                probabilities = torch.nn.functional.softmax(logits, dim=1).cpu().numpy()

                return probabilities

            except Exception as e:
                # å¦‚æœé¢„æµ‹å¤±è´¥ï¼Œè¿”å›å‡åŒ€åˆ†å¸ƒ
                n_classes = len(torch.unique(self.support_labels))
                return np.full((len(X), n_classes), 1.0 / n_classes)

# ==================== CNNå’ŒLSTMæ¨¡å‹ ====================
class CNNClassifier(nn.Module):
    """é˜²è¿‡æ‹Ÿåˆçš„1D CNNåˆ†ç±»å™¨"""
    def __init__(self, input_dim, n_classes=3, dropout_rate=0.5):
        super(CNNClassifier, self).__init__()

        self.input_dim = input_dim

        # ç®€åŒ–çš„å·ç§¯å±‚ - å‡å°‘å‚æ•°
        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, padding=1)  # å‡å°‘é€šé“æ•°
        self.conv2 = nn.Conv1d(16, 16, kernel_size=3, padding=1) # ä¿æŒé€šé“æ•°

        # æ± åŒ–å±‚
        self.pool = nn.AdaptiveAvgPool1d(4)  # å‡å°‘æ± åŒ–åçš„é•¿åº¦

        # å…¨è¿æ¥å±‚ - å¤§å¹…ç®€åŒ–
        self.fc1 = nn.Linear(16 * 4, 16)     # å‡å°‘éšè—å±‚å¤§å°
        self.fc2 = nn.Linear(16, n_classes)

        # å¢å¼ºæ­£åˆ™åŒ–
        self.dropout = nn.Dropout(dropout_rate)
        self.batch_norm1 = nn.BatchNorm1d(16)
        self.batch_norm2 = nn.BatchNorm1d(16)
        self.relu = nn.ReLU()

    def forward(self, x):
        # x: [batch_size, input_dim] -> [batch_size, 1, input_dim]
        x = x.unsqueeze(1)

        # å·ç§¯å±‚ + æ‰¹å½’ä¸€åŒ– + Dropout
        x = self.relu(self.batch_norm1(self.conv1(x)))
        x = self.dropout(x)
        x = self.relu(self.batch_norm2(self.conv2(x)))
        x = self.dropout(x)

        # æ± åŒ–
        x = self.pool(x)

        # å±•å¹³
        x = x.view(x.size(0), -1)

        # å…¨è¿æ¥å±‚ + å¼ºDropout
        x = self.dropout(self.relu(self.fc1(x)))
        x = self.fc2(x)

        return x

class LSTMClassifier(nn.Module):
    """é˜²è¿‡æ‹Ÿåˆçš„LSTMåˆ†ç±»å™¨"""
    def __init__(self, input_dim, hidden_dim=32, n_layers=1, n_classes=3, dropout_rate=0.5):
        super(LSTMClassifier, self).__init__()

        self.input_dim = input_dim
        self.hidden_dim = hidden_dim
        self.n_layers = n_layers

        # ç®€åŒ–çš„LSTMå±‚ - å‡å°‘å‚æ•°
        self.lstm = nn.LSTM(1, hidden_dim, n_layers,
                           batch_first=True, dropout=0)  # å•å±‚LSTMä¸ä½¿ç”¨å†…ç½®dropout

        # å…¨è¿æ¥å±‚ - ç®€åŒ–
        self.fc1 = nn.Linear(hidden_dim, 16)      # æ·»åŠ ä¸­é—´å±‚
        self.fc2 = nn.Linear(16, n_classes)       # è¾“å‡ºå±‚

        # å¢å¼ºæ­£åˆ™åŒ–
        self.dropout = nn.Dropout(dropout_rate)
        self.layer_norm = nn.LayerNorm(hidden_dim)
        self.relu = nn.ReLU()

    def forward(self, x):
        # x: [batch_size, input_dim] -> [batch_size, input_dim, 1]
        x = x.unsqueeze(-1)

        # LSTM
        lstm_out, _ = self.lstm(x)

        # ä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º + LayerNorm
        last_output = self.layer_norm(lstm_out[:, -1, :])

        # åˆ†ç±» - ä¸¤å±‚å…¨è¿æ¥ + å¼ºDropout
        output = self.dropout(self.relu(self.fc1(last_output)))
        output = self.fc2(output)

        return output

class DeepClassifierWrapper:
    """æ·±åº¦å­¦ä¹ åˆ†ç±»å™¨åŒ…è£…ç±» - å®Œå…¨å…¼å®¹sklearnæ¥å£"""
    def __init__(self, model_type='cnn', input_dim=20, n_classes=3,
                 epochs=30, learning_rate=0.001, device='cuda'):
        self.model_type = model_type
        self.input_dim = input_dim
        self.n_classes = n_classes
        self.epochs = epochs
        self.learning_rate = learning_rate
        self.device = device if torch.cuda.is_available() else 'cpu'

    def get_params(self, deep=True):
        """è·å–å‚æ•° - sklearnå…¼å®¹æ€§è¦æ±‚"""
        return {
            'model_type': self.model_type,
            'input_dim': self.input_dim,
            'n_classes': self.n_classes,
            'epochs': self.epochs,
            'learning_rate': self.learning_rate,
            'device': self.device
        }

    def set_params(self, **params):
        """è®¾ç½®å‚æ•° - sklearnå…¼å®¹æ€§è¦æ±‚"""
        for key, value in params.items():
            setattr(self, key, value)
        return self

    def _initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        # åˆ›å»ºæ¨¡å‹
        if self.model_type == 'cnn':
            self.model = CNNClassifier(self.input_dim, self.n_classes).to(self.device)
        elif self.model_type == 'lstm':
            self.model = LSTMClassifier(self.input_dim, n_classes=self.n_classes).to(self.device)
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")

        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)
        self.criterion = nn.CrossEntropyLoss()
        self.is_fitted = False

    def fit(self, X, y):
        """è®­ç»ƒæ¨¡å‹ - æ·»åŠ æ—©åœæœºåˆ¶é˜²æ­¢è¿‡æ‹Ÿåˆ"""
        # åˆå§‹åŒ–æ¨¡å‹ï¼ˆå¦‚æœè¿˜æ²¡æœ‰åˆå§‹åŒ–ï¼‰
        if not hasattr(self, 'model'):
            self._initialize_model()

        self.model.train()

        X_tensor = torch.FloatTensor(X).to(self.device)
        y_tensor = torch.LongTensor(y).to(self.device)

        # åˆ†å‰²è®­ç»ƒå’ŒéªŒè¯é›†
        if len(X) > 10:
            val_size = max(2, len(X) // 5)  # 20%ä½œä¸ºéªŒè¯é›†
            indices = torch.randperm(len(X))
            train_indices = indices[val_size:]
            val_indices = indices[:val_size]

            X_train, X_val = X_tensor[train_indices], X_tensor[val_indices]
            y_train, y_val = y_tensor[train_indices], y_tensor[val_indices]
        else:
            # æ•°æ®å¤ªå°‘ï¼Œä¸åˆ†å‰²éªŒè¯é›†
            X_train, X_val = X_tensor, X_tensor
            y_train, y_val = y_tensor, y_tensor

        dataset = TensorDataset(X_train, y_train)
        dataloader = DataLoader(dataset, batch_size=min(4, len(X_train)), shuffle=True)

        # æ—©åœå‚æ•°
        best_val_loss = float('inf')
        patience = 5
        patience_counter = 0

        for epoch in range(self.epochs):
            # è®­ç»ƒ
            self.model.train()
            epoch_loss = 0
            for batch_x, batch_y in dataloader:
                self.optimizer.zero_grad()

                outputs = self.model(batch_x)
                loss = self.criterion(outputs, batch_y)

                loss.backward()
                # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                self.optimizer.step()

                epoch_loss += loss.item()

            # éªŒè¯
            self.model.eval()
            with torch.no_grad():
                val_outputs = self.model(X_val)
                val_loss = self.criterion(val_outputs, y_val).item()

            # æ—©åœæ£€æŸ¥
            if val_loss < best_val_loss:
                best_val_loss = val_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€
                best_model_state = self.model.state_dict().copy()
            else:
                patience_counter += 1

            if patience_counter >= patience:
                # æ¢å¤æœ€ä½³æ¨¡å‹
                self.model.load_state_dict(best_model_state)
                break

            if (epoch + 1) % 10 == 0:
                avg_loss = epoch_loss / len(dataloader)
                print(f"   {self.model_type.upper()} Epoch {epoch+1}/{self.epochs}, Loss: {avg_loss:.4f}")

        self.is_fitted = True
        return self

    def predict(self, X):
        """é¢„æµ‹"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")

        if not hasattr(self, 'model'):
            raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")

        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(self.device)
            outputs = self.model(X_tensor)
            predictions = torch.argmax(outputs, dim=1).cpu().numpy()

        return predictions

    def predict_proba(self, X):
        """é¢„æµ‹æ¦‚ç‡"""
        if not self.is_fitted:
            raise ValueError("æ¨¡å‹æœªè®­ç»ƒ")

        if not hasattr(self, 'model'):
            raise ValueError("æ¨¡å‹æœªåˆå§‹åŒ–")

        self.model.eval()
        with torch.no_grad():
            X_tensor = torch.FloatTensor(X).to(self.device)
            outputs = self.model(X_tensor)
            probabilities = torch.softmax(outputs, dim=1).cpu().numpy()

        return probabilities

class FeatureLearner:
    """ç±»åˆ«ç‰¹å¾å­¦ä¹ å™¨"""
    
    def __init__(self, n_components=10):
        self.n_components = n_components
        self.class_features = {}
        self.class_scalers = {}
        self.class_pcas = {}
        
    def learn_class_features(self, data_by_class):
        """å­¦ä¹ æ¯ä¸ªç±»åˆ«çš„ç‰¹å¾åˆ†å¸ƒ"""
        print("ğŸ§  å­¦ä¹ å„ç±»åˆ«ç‰¹å¾åˆ†å¸ƒ...")
        
        for class_label, class_data in data_by_class.items():
            if len(class_data) == 0:
                continue
                
            print(f"   ğŸ“Š å­¦ä¹ ç±»åˆ« {class_label} ç‰¹å¾ ({len(class_data)} ä¸ªæ ·æœ¬)")
            
            # å±•å¹³æ•°æ®
            flattened_data = np.array([sample.flatten() for sample in class_data])
            
            # æ ‡å‡†åŒ–
            scaler = StandardScaler()
            scaled_data = scaler.fit_transform(flattened_data)
            
            # PCAé™ç»´æå–ä¸»è¦ç‰¹å¾
            n_comp = min(self.n_components, len(class_data)-1, scaled_data.shape[1])
            pca = PCA(n_components=n_comp)
            pca_features = pca.fit_transform(scaled_data)
            
            # è®¡ç®—ç‰¹å¾ç»Ÿè®¡ä¿¡æ¯
            feature_stats = {
                'mean': np.mean(pca_features, axis=0),
                'std': np.std(pca_features, axis=0),
                'min': np.min(pca_features, axis=0),
                'max': np.max(pca_features, axis=0),
                'median': np.median(pca_features, axis=0)
            }
            
            # ä½¿ç”¨K-meansèšç±»å‘ç°å­æ¨¡å¼
            if len(class_data) >= 3:
                n_clusters = min(3, len(class_data))
                kmeans = KMeans(n_clusters=n_clusters, random_state=42)
                clusters = kmeans.fit_predict(pca_features)
                cluster_centers = kmeans.cluster_centers_
            else:
                clusters = np.zeros(len(class_data))
                cluster_centers = np.array([feature_stats['mean']])
            
            self.class_features[class_label] = {
                'stats': feature_stats,
                'clusters': clusters,
                'cluster_centers': cluster_centers,
                'raw_features': pca_features
            }
            self.class_scalers[class_label] = scaler
            self.class_pcas[class_label] = pca
            
            print(f"     âœ… æå– {n_comp} ä¸ªä¸»è¦ç‰¹å¾")
    
    def generate_perturbation_patterns(self, class_label, n_patterns=5):
        """ä¸ºæŒ‡å®šç±»åˆ«ç”Ÿæˆæ‰°åŠ¨æ¨¡å¼"""
        if class_label not in self.class_features:
            return []
        
        features = self.class_features[class_label]
        stats = features['stats']
        
        patterns = []
        
        # æ¨¡å¼1: åŸºäºå‡å€¼å’Œæ ‡å‡†å·®çš„é«˜æ–¯æ‰°åŠ¨
        for i in range(n_patterns):
            noise_scale = 0.1 + i * 0.05  # é€’å¢çš„å™ªå£°å¼ºåº¦
            pattern = {
                'type': 'gaussian',
                'mean': stats['mean'],
                'std': stats['std'] * noise_scale,
                'intensity': noise_scale
            }
            patterns.append(pattern)
        
        # æ¨¡å¼2: åŸºäºèšç±»ä¸­å¿ƒçš„æ‰°åŠ¨
        for center in features['cluster_centers']:
            pattern = {
                'type': 'cluster_based',
                'center': center,
                'std': stats['std'] * 0.15,
                'intensity': 0.15
            }
            patterns.append(pattern)
        
        return patterns

class SimpleVAEGAN(nn.Module):
    """ç®€åŒ–çš„VAE-GANç½‘ç»œ"""
    
    def __init__(self, input_dim, latent_dim=32, hidden_dim=128):
        super(SimpleVAEGAN, self).__init__()
        self.input_dim = input_dim
        self.latent_dim = latent_dim
        
        # VAEç¼–ç å™¨
        self.encoder = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.ReLU()
        )
        self.mu_layer = nn.Linear(hidden_dim//2, latent_dim)
        self.logvar_layer = nn.Linear(hidden_dim//2, latent_dim)
        
        # VAEè§£ç å™¨
        self.decoder = nn.Sequential(
            nn.Linear(latent_dim, hidden_dim//2),
            nn.ReLU(),
            nn.Linear(hidden_dim//2, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, input_dim),
            nn.Tanh()
        )
        
        # åˆ¤åˆ«å™¨
        self.discriminator = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim, hidden_dim//2),
            nn.LeakyReLU(0.2),
            nn.Linear(hidden_dim//2, 1),
            nn.Sigmoid()
        )
    
    def encode(self, x):
        h = self.encoder(x)
        mu = self.mu_layer(h)
        logvar = self.logvar_layer(h)
        return mu, logvar
    
    def reparameterize(self, mu, logvar):
        std = torch.exp(0.5 * logvar)
        eps = torch.randn_like(std)
        return mu + eps * std
    
    def decode(self, z):
        return self.decoder(z)
    
    def forward(self, x):
        mu, logvar = self.encode(x)
        z = self.reparameterize(mu, logvar)
        recon = self.decode(z)
        return recon, mu, logvar
    
    def discriminate(self, x):
        return self.discriminator(x)

class FeatureBasedAugmentationSystem:
    """åŸºäºç‰¹å¾å­¦ä¹ çš„æ•°æ®å¢å¼ºç³»ç»Ÿ"""
    
    def __init__(self, seq_len=400, latent_dim=32, device='cuda'):
        self.seq_len = seq_len
        self.latent_dim = latent_dim
        self.device = torch.device(device if torch.cuda.is_available() else 'cpu')
        
        self.feature_learner = FeatureLearner()
        self.vae_gan = None
        self.global_scaler = MinMaxScaler(feature_range=(0.01, 0.99))
        
        print(f"ğŸ”§ åˆå§‹åŒ–å¢å¼ºç³»ç»Ÿï¼Œè®¾å¤‡: {self.device}")
    
    def load_and_process_data(self, data_dir):
        """æ™ºèƒ½åŠ è½½å’Œå¤„ç†æ•°æ® - è‡ªåŠ¨æ£€æµ‹æ•°æ®ç±»å‹"""
        print(f"ğŸ“‚ ä» {data_dir} åŠ è½½æ•°æ®...")

        if not os.path.exists(data_dir):
            raise ValueError(f"ç›®å½• '{data_dir}' ä¸å­˜åœ¨")

        data_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]
        if not data_files:
            raise ValueError(f"åœ¨ {data_dir} ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")

        # æ£€æµ‹æ•°æ®ç±»å‹
        sample_file = os.path.join(data_dir, data_files[0])
        sample_df = pd.read_csv(sample_file)

        # åˆ¤æ–­æ˜¯å¦ä¸ºgaitç±»å‹æ•°æ®
        is_gait_data = (
            len(sample_df) < 20 and  # è¡Œæ•°å°‘
            sample_df.shape[1] <= 5 and  # åˆ—æ•°å°‘
            any(col in sample_df.columns for col in ['step_length', 'step_frequency', 'step_speed'])
        )

        if is_gait_data:
            print("ğŸš¶ æ£€æµ‹åˆ°æ­¥æ€ç‰¹å¾æ•°æ®ï¼Œä½¿ç”¨ä¸“é—¨çš„å¤„ç†æ–¹æ³•")
            return self._load_gait_data(data_dir, data_files)
        else:
            print("ğŸ“Š æ£€æµ‹åˆ°æ—¶åºæ•°æ®ï¼Œä½¿ç”¨æ ‡å‡†å¤„ç†æ–¹æ³•")
            return self._load_timeseries_data(data_dir, data_files)

    def _load_gait_data(self, data_dir, data_files):
        """ä¸“é—¨å¤„ç†æ­¥æ€ç‰¹å¾æ•°æ®"""
        all_data = []
        all_labels = []
        data_by_class = {0: [], 1: [], 2: []}

        print(f"ğŸ“Š æ‰¾åˆ° {len(data_files)} ä¸ªæ­¥æ€æ•°æ®æ–‡ä»¶")

        for file in data_files:
            try:
                df = pd.read_csv(os.path.join(data_dir, file))

                # æå–æ ‡ç­¾ - ä¼˜å…ˆä½¿ç”¨CSVä¸­çš„æ ‡ç­¾åˆ—
                if 'label' in df.columns:
                    label = int(df['label'].iloc[0])
                    df = df.drop('label', axis=1)
                    print(f"   âœ… ä»CSVæ–‡ä»¶ {file} è¯»å–æ ‡ç­¾: {label}")
                else:
                    # å¦‚æœCSVä¸­æ²¡æœ‰æ ‡ç­¾åˆ—ï¼Œåˆ™ä»æ–‡ä»¶åæå–ï¼ˆä½†è¿™ä¸åº”è¯¥å‘ç”Ÿï¼‰
                    print(f"   âš ï¸ è­¦å‘Šï¼šCSVæ–‡ä»¶ {file} ä¸­æ²¡æœ‰æ ‡ç­¾åˆ—ï¼Œå°è¯•ä»æ–‡ä»¶åæå–")
                    import re
                    numbers = re.findall(r'\d+', file)
                    if numbers:
                        # æå–æ–‡ä»¶ç¼–å·ï¼Œä½†ä¸ç›´æ¥ç”¨ä½œæ ‡ç­¾
                        file_num = int(numbers[0])
                        # é»˜è®¤æ ‡ç­¾æ˜ å°„ï¼ˆè¿™æ˜¯å¤‡ç”¨æ–¹æ¡ˆï¼Œä¸åº”è¯¥è¢«ä½¿ç”¨ï¼‰
                        label = file_num % 3
                        print(f"   âš ï¸ ä»æ–‡ä»¶åæå–ç¼–å· {file_num}ï¼Œæ˜ å°„ä¸ºæ ‡ç­¾ {label}")
                    else:
                        label = 0
                        print(f"   âŒ æ— æ³•ä»æ–‡ä»¶å {file} æå–ä»»ä½•ä¿¡æ¯ï¼Œä½¿ç”¨é»˜è®¤æ ‡ç­¾0")

                # æ­¥æ€æ•°æ®ç‰¹æ®Šå¤„ç†ï¼šä¸å¡«å……ï¼Œç›´æ¥ä½¿ç”¨ç»Ÿè®¡ç‰¹å¾
                data = df.values.astype(np.float32)
                data = np.nan_to_num(data, nan=0.0, posinf=1.0, neginf=-1.0)

                # è®¡ç®—ç»Ÿè®¡ç‰¹å¾è€Œéå¡«å……
                if len(data) > 0:
                    # å¯¹æ¯åˆ—è®¡ç®—ç»Ÿè®¡ç‰¹å¾
                    features = []
                    for col in range(data.shape[1]):
                        col_data = data[:, col]
                        # åŸºæœ¬ç»Ÿè®¡é‡
                        features.extend([
                            np.mean(col_data),
                            np.std(col_data),
                            np.min(col_data),
                            np.max(col_data),
                            np.median(col_data)
                        ])

                    # è½¬æ¢ä¸ºå›ºå®šé•¿åº¦çš„ç‰¹å¾å‘é‡
                    feature_vector = np.array(features, dtype=np.float32)

                    # é‡å¡‘ä¸º(1, n_features)ä»¥ä¿æŒä¸€è‡´æ€§
                    data = feature_vector.reshape(1, -1)
                else:
                    # å¦‚æœæ•°æ®ä¸ºç©ºï¼Œåˆ›å»ºé›¶å‘é‡
                    n_features = df.shape[1] * 5  # æ¯åˆ—5ä¸ªç»Ÿè®¡ç‰¹å¾
                    data = np.zeros((1, n_features), dtype=np.float32)

                all_data.append(data)
                all_labels.append(label)

                # æŒ‰ç±»åˆ«åˆ†ç»„
                if label in data_by_class:
                    data_by_class[label].append(data)

            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ­¥æ€æ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}")
                continue

        if not all_data:
            raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ­¥æ€æ•°æ®æ–‡ä»¶")

        print(f"âœ… æ­¥æ€æ•°æ®åŠ è½½å®Œæˆ")
        print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {len(all_data)}")
        print(f"ğŸ“Š æ¯ä¸ªæ ·æœ¬å½¢çŠ¶: {all_data[0].shape}")
        print(f"ğŸ·ï¸ æ ‡ç­¾åˆ†å¸ƒ: {[len(data_by_class[i]) for i in range(3)]}")

        return all_data, np.array(all_labels), data_by_class

    def _load_timeseries_data(self, data_dir, data_files):
        """å¤„ç†æ—¶åºæ•°æ®ï¼ˆåŸæœ‰é€»è¾‘ï¼‰"""
        all_data = []
        all_labels = []
        data_by_class = {0: [], 1: [], 2: []}

        print(f"ğŸ“Š æ‰¾åˆ° {len(data_files)} ä¸ªæ—¶åºæ•°æ®æ–‡ä»¶")

        for file in data_files:
            try:
                df = pd.read_csv(os.path.join(data_dir, file))

                # æå–æ ‡ç­¾ - ä¿®å¤æ ‡ç­¾æå–é€»è¾‘
                if 'label' in df.columns:
                    label = int(df['label'].iloc[0])
                    df = df.drop('label', axis=1)
                else:
                    # ä»æ–‡ä»¶åæå–æ ‡ç­¾ - ä¿®å¤é”™è¯¯çš„æ¨¡è¿ç®—
                    import re
                    numbers = re.findall(r'\d+', file)
                    if numbers:
                        # å‡è®¾æ–‡ä»¶åæ ¼å¼ä¸º "classX_..." æˆ– "X_..."
                        first_num = int(numbers[0])
                        # å¦‚æœæ•°å­—æ˜¯0,1,2ç›´æ¥ä½¿ç”¨ï¼›å¦‚æœæ˜¯1,2,3è½¬æ¢ä¸º0,1,2
                        if first_num in [0, 1, 2]:
                            label = first_num
                        elif first_num in [1, 2, 3]:
                            label = first_num - 1
                        else:
                            # å…¶ä»–æƒ…å†µä½¿ç”¨æ¨¡è¿ç®—ï¼Œä½†ç¡®ä¿ç»“æœåœ¨0-2èŒƒå›´å†…
                            label = first_num % 3
                    else:
                        label = 0
                        print(f"   âš ï¸ æ— æ³•ä»æ–‡ä»¶å {file} æå–æ ‡ç­¾ï¼Œä½¿ç”¨é»˜è®¤æ ‡ç­¾0")

                # æ•°æ®é¢„å¤„ç†
                data = df.values.astype(np.float32)
                data = np.nan_to_num(data, nan=0.0, posinf=1.0, neginf=-1.0)

                # æˆªæ–­æˆ–å¡«å……åˆ°æŒ‡å®šé•¿åº¦
                if len(data) > self.seq_len:
                    data = data[:self.seq_len]
                else:
                    padding = np.zeros((self.seq_len - len(data), data.shape[1]))
                    data = np.vstack([data, padding])

                all_data.append(data)
                all_labels.append(label)

                # æŒ‰ç±»åˆ«åˆ†ç»„
                if label in data_by_class:
                    data_by_class[label].append(data)

            except Exception as e:
                print(f"âš ï¸ å¤„ç†æ—¶åºæ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}")
                continue

        if not all_data:
            raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ—¶åºæ•°æ®æ–‡ä»¶")

        print(f"âœ… æ—¶åºæ•°æ®åŠ è½½å®Œæˆ")
        print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {len(all_data)}")
        print(f"ğŸ“Š æ¯ä¸ªæ ·æœ¬å½¢çŠ¶: {all_data[0].shape}")
        print(f"ğŸ·ï¸ æ ‡ç­¾åˆ†å¸ƒ: {[len(data_by_class[i]) for i in range(3)]}")

        return all_data, np.array(all_labels), data_by_class
    
    def apply_feature_based_perturbation(self, data, class_label, perturbation_pattern):
        """åº”ç”¨åŸºäºç‰¹å¾å­¦ä¹ çš„æ•°æ®æ‰°åŠ¨"""
        if class_label not in self.feature_learner.class_scalers:
            return data
        
        # è·å–å¯¹åº”çš„scalerå’Œpca
        scaler = self.feature_learner.class_scalers[class_label]
        pca = self.feature_learner.class_pcas[class_label]
        
        # å±•å¹³å¹¶æ ‡å‡†åŒ–
        flat_data = data.flatten().reshape(1, -1)
        scaled_data = scaler.transform(flat_data)
        
        # PCAå˜æ¢åˆ°ç‰¹å¾ç©ºé—´
        pca_data = pca.transform(scaled_data)
        
        # åº”ç”¨æ‰°åŠ¨
        if perturbation_pattern['type'] == 'gaussian':
            noise = np.random.normal(0, perturbation_pattern['std'], pca_data.shape)
            perturbed_pca = pca_data + noise
        elif perturbation_pattern['type'] == 'cluster_based':
            direction = perturbation_pattern['center'] - pca_data[0]
            noise = np.random.normal(0, perturbation_pattern['std'], pca_data.shape)
            perturbed_pca = pca_data + 0.3 * direction + noise
        else:
            perturbed_pca = pca_data
        
        # é€†å˜æ¢å›åŸå§‹ç©ºé—´
        perturbed_scaled = pca.inverse_transform(perturbed_pca)
        perturbed_flat = scaler.inverse_transform(perturbed_scaled)
        
        # é‡å¡‘å›åŸå§‹å½¢çŠ¶
        perturbed_data = perturbed_flat.reshape(data.shape)
        
        return perturbed_data

    def train_vae_gan(self, data_list, labels, epochs=100, batch_size=16):
        """è®­ç»ƒæ”¹è¿›çš„VAE-GANç½‘ç»œ"""
        print("ğŸ”§ è®­ç»ƒæ”¹è¿›çš„VAE-GANç½‘ç»œ...")

        # å‡†å¤‡è®­ç»ƒæ•°æ®
        flattened_data = np.array([data.flatten() for data in data_list])

        # å…¨å±€æ ‡å‡†åŒ–
        scaled_data = self.global_scaler.fit_transform(flattened_data)

        # è½¬æ¢ä¸ºPyTorchå¼ é‡
        tensor_data = torch.FloatTensor(scaled_data).to(self.device)
        tensor_labels = torch.LongTensor(labels).to(self.device)

        dataset = TensorDataset(tensor_data, tensor_labels)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, drop_last=True)

        # åˆå§‹åŒ–VAE-GAN
        input_dim = scaled_data.shape[1]
        self.vae_gan = SimpleVAEGAN(input_dim, self.latent_dim).to(self.device)

        # æ”¹è¿›çš„ä¼˜åŒ–å™¨è®¾ç½® - ä¸åŒå­¦ä¹ ç‡
        optimizer_vae = optim.Adam(
            list(self.vae_gan.encoder.parameters()) + list(self.vae_gan.decoder.parameters()),
            lr=2e-4, betas=(0.5, 0.999)  # ç”Ÿæˆå™¨å­¦ä¹ ç‡ç¨é«˜
        )
        optimizer_disc = optim.Adam(
            self.vae_gan.discriminator.parameters(),
            lr=1e-4, betas=(0.5, 0.999)  # åˆ¤åˆ«å™¨å­¦ä¹ ç‡è¾ƒä½
        )

        # æŸå¤±å‡½æ•°
        mse_loss = nn.MSELoss()
        bce_loss = nn.BCELoss()

        print(f"   ğŸ“Š è®­ç»ƒæ•°æ®å½¢çŠ¶: {scaled_data.shape}")
        print(f"   ğŸ”§ ç½‘ç»œè¾“å…¥ç»´åº¦: {input_dim}")

        # è®­ç»ƒå†å²è®°å½•å’Œæ—©åœæœºåˆ¶
        vae_losses = []
        disc_losses = []
        best_vae_loss = float('inf')
        patience = 50
        patience_counter = 0

        for epoch in range(epochs):
            epoch_vae_loss = 0
            epoch_disc_loss = 0

            for batch_idx, (batch_data, batch_labels) in enumerate(dataloader):
                batch_size_actual = batch_data.size(0)

                # åŠ¨æ€è°ƒæ•´è®­ç»ƒé¢‘ç‡ - é˜²æ­¢åˆ¤åˆ«å™¨è¿‡å¼º
                train_disc = (batch_idx % 2 == 0) or (epoch < epochs // 4)

                # è®­ç»ƒVAE (æ¯ä¸ªbatchéƒ½è®­ç»ƒ)
                optimizer_vae.zero_grad()

                recon_data, mu, logvar = self.vae_gan(batch_data)

                # VAEæŸå¤± - è°ƒæ•´æƒé‡
                recon_loss = mse_loss(recon_data, batch_data)
                kl_loss = -0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())
                kl_loss /= batch_size_actual * input_dim

                # å¯¹æŠ—æŸå¤± - å¢åŠ æƒé‡
                fake_validity = self.vae_gan.discriminate(recon_data)
                adv_loss = bce_loss(fake_validity, torch.ones_like(fake_validity))

                # è°ƒæ•´æŸå¤±æƒé‡
                beta_kl = min(1.0, epoch / (epochs * 0.5))  # KLæƒé‡é€æ¸å¢åŠ 
                alpha_adv = 0.1 if epoch < epochs // 2 else 0.05  # å¯¹æŠ—æƒé‡åæœŸé™ä½

                vae_loss = recon_loss + beta_kl * 0.1 * kl_loss + alpha_adv * adv_loss
                vae_loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(
                    list(self.vae_gan.encoder.parameters()) + list(self.vae_gan.decoder.parameters()),
                    max_norm=1.0
                )

                optimizer_vae.step()

                # è®­ç»ƒåˆ¤åˆ«å™¨ (é™ä½é¢‘ç‡)
                if train_disc:
                    optimizer_disc.zero_grad()

                    real_validity = self.vae_gan.discriminate(batch_data)
                    fake_validity = self.vae_gan.discriminate(recon_data.detach())

                    # æ ‡ç­¾å¹³æ»‘
                    real_labels = torch.ones_like(real_validity) * 0.9
                    fake_labels = torch.zeros_like(fake_validity) + 0.1

                    real_loss = bce_loss(real_validity, real_labels)
                    fake_loss = bce_loss(fake_validity, fake_labels)
                    disc_loss = (real_loss + fake_loss) / 2

                    disc_loss.backward()

                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(
                        self.vae_gan.discriminator.parameters(),
                        max_norm=1.0
                    )

                    optimizer_disc.step()
                    epoch_disc_loss += disc_loss.item()

                epoch_vae_loss += vae_loss.item()

            # è®°å½•æŸå¤±
            vae_losses.append(epoch_vae_loss / len(dataloader))
            disc_losses.append(epoch_disc_loss / len(dataloader) if epoch_disc_loss > 0 else 0)

            # æ—©åœæ£€æŸ¥
            current_vae_loss = vae_losses[-1]
            if current_vae_loss < best_vae_loss:
                best_vae_loss = current_vae_loss
                patience_counter = 0
                # ä¿å­˜æœ€ä½³æ¨¡å‹çŠ¶æ€
                best_model_state = {
                    'encoder': self.vae_gan.encoder.state_dict(),
                    'decoder': self.vae_gan.decoder.state_dict(),
                    'discriminator': self.vae_gan.discriminator.state_dict()
                }
            else:
                patience_counter += 1

            # æ‰“å°è¿›åº¦å’ŒæŸå¤±è¶‹åŠ¿åˆ†æ
            if (epoch + 1) % 50 == 0:
                recent_vae = np.mean(vae_losses[-10:])
                recent_disc = np.mean(disc_losses[-10:])

                print(f"   Epoch {epoch+1}/{epochs}:")
                print(f"     VAE Loss: {recent_vae:.4f}, Disc Loss: {recent_disc:.4f}")
                print(f"     Best VAE Loss: {best_vae_loss:.4f}, Patience: {patience_counter}/{patience}")

                # è®­ç»ƒç¨³å®šæ€§æ£€æŸ¥ï¼ˆç®€åŒ–è¾“å‡ºï¼‰
                if len(vae_losses) >= 20:
                    disc_trend = np.polyfit(range(10), disc_losses[-10:], 1)[0]
                    # è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´
                    if disc_trend > 0.02:
                        for param_group in optimizer_disc.param_groups:
                            param_group['lr'] *= 0.9

            # æ—©åœæ¡ä»¶
            if patience_counter >= patience:
                print(f"   ğŸ›‘ æ—©åœè§¦å‘ï¼åœ¨epoch {epoch+1}åœæ­¢è®­ç»ƒ")
                # æ¢å¤æœ€ä½³æ¨¡å‹
                if 'best_model_state' in locals():
                    self.vae_gan.encoder.load_state_dict(best_model_state['encoder'])
                    self.vae_gan.decoder.load_state_dict(best_model_state['decoder'])
                    self.vae_gan.discriminator.load_state_dict(best_model_state['discriminator'])
                    print(f"   âœ… æ¢å¤æœ€ä½³æ¨¡å‹çŠ¶æ€ (VAE Loss: {best_vae_loss:.4f})")
                break

        print("âœ… VAE-GANè®­ç»ƒå®Œæˆ")
        print(f"   ğŸ“Š æœ€ç»ˆVAEæŸå¤±: {vae_losses[-1]:.4f}")
        print(f"   ğŸ“Š æœ€ç»ˆåˆ¤åˆ«å™¨æŸå¤±: {disc_losses[-1]:.4f}")

        return vae_losses, disc_losses

    def generate_feature_perturbation_samples(self, original_data, original_labels, n_augment_per_sample=2):
        """æ–¹æ¡ˆ1: ä»…ç‰¹å¾æ‰°åŠ¨å¢å¼º"""
        print("ğŸ”„ æ‰§è¡Œæ–¹æ¡ˆ1: ç‰¹å¾æ‰°åŠ¨å¢å¼º...")

        augmented_data = []
        augmented_labels = []

        for data, label in zip(original_data, original_labels):
            perturbation_patterns = self.feature_learner.generate_perturbation_patterns(label)

            for j in range(n_augment_per_sample):
                if perturbation_patterns:
                    pattern = perturbation_patterns[j % len(perturbation_patterns)]
                    # é€‚ä¸­çš„æ‰°åŠ¨å¼ºåº¦
                    pattern['intensity'] *= 0.7
                    enhanced_data = self.apply_feature_based_perturbation(data, label, pattern)
                else:
                    # è½»å¾®é«˜æ–¯å™ªå£°
                    enhanced_data = data + np.random.normal(0, 0.02, data.shape)

                if self._is_valid_sample(enhanced_data, data):
                    augmented_data.append(enhanced_data)
                    augmented_labels.append(label)

        print(f"âœ… ç‰¹å¾æ‰°åŠ¨å¢å¼ºå®Œæˆï¼Œç”Ÿæˆ {len(augmented_data)} ä¸ªæ ·æœ¬")
        return augmented_data, np.array(augmented_labels)

    def generate_vae_reconstruction_samples(self, original_data, original_labels, n_augment_per_sample=2, mix_ratio=0.8):
        """æ–¹æ¡ˆ2: VAEé‡æ„å¢å¼º"""
        print("ğŸ”„ æ‰§è¡Œæ–¹æ¡ˆ2: VAEé‡æ„å¢å¼º...")

        if self.vae_gan is None:
            raise ValueError("VAE-GANæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train_vae_gan")

        augmented_data = []
        augmented_labels = []

        # è°ƒè¯•è®¡æ•°å™¨
        total_attempts = 0
        valid_samples = 0
        conservative_samples = 0
        fallback_samples = 0

        with torch.no_grad():
            for i, (data, label) in enumerate(zip(original_data, original_labels)):
                print(f"   å¤„ç†æ ·æœ¬ {i+1}/{len(original_data)}, æ ‡ç­¾: {label}")

                for j in range(n_augment_per_sample):
                    total_attempts += 1

                    try:
                        # æ·»åŠ è½»å¾®å™ªå£°åˆ°åŸå§‹æ•°æ®
                        noise_scale = 0.01 * (j + 1)
                        noisy_data = data + np.random.normal(0, noise_scale, data.shape)

                        # VAEé‡æ„
                        flat_data = noisy_data.flatten().reshape(1, -1)
                        scaled_data = self.global_scaler.transform(flat_data)
                        tensor_data = torch.FloatTensor(scaled_data).to(self.device)

                        recon_data, _, _ = self.vae_gan(tensor_data)

                        # ä¸åŸå§‹æ•°æ®æ··åˆ
                        mixed_data = mix_ratio * tensor_data + (1 - mix_ratio) * recon_data

                        enhanced_scaled = mixed_data.cpu().numpy()
                        enhanced_flat = self.global_scaler.inverse_transform(enhanced_scaled)
                        enhanced_data = enhanced_flat.reshape(data.shape)

                        # æš‚æ—¶è·³è¿‡è´¨é‡æ£€æŸ¥ï¼Œç›´æ¥æ¥å—æ‰€æœ‰æ ·æœ¬
                        augmented_data.append(enhanced_data)
                        augmented_labels.append(label)
                        valid_samples += 1

                    except Exception as e:
                        print(f"   âš ï¸ æ ·æœ¬ {i+1}-{j+1} å¤„ç†å¤±è´¥: {str(e)}")
                        # ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ
                        fallback_data = data + np.random.normal(0, 0.005, data.shape)
                        augmented_data.append(fallback_data)
                        augmented_labels.append(label)
                        fallback_samples += 1

        print(f"âœ… VAEé‡æ„å¢å¼ºå®Œæˆï¼Œç”Ÿæˆ {len(augmented_data)} ä¸ªæ ·æœ¬")
        print(f"   ğŸ“Š ç»Ÿè®¡: æ€»å°è¯• {total_attempts}, æœ‰æ•ˆ {valid_samples}, ä¿å®ˆ {conservative_samples}, å¤‡ç”¨ {fallback_samples}")
        return augmented_data, np.array(augmented_labels)

    def generate_hybrid_samples(self, original_data, original_labels, n_augment_per_sample=2):
        """æ–¹æ¡ˆ3: æ··åˆå¢å¼º (ç‰¹å¾æ‰°åŠ¨ + VAEå¢å¼º)"""
        print("ğŸ”„ æ‰§è¡Œæ–¹æ¡ˆ3: æ··åˆå¢å¼º...")

        if self.vae_gan is None:
            raise ValueError("VAE-GANæœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨train_vae_gan")

        augmented_data = []
        augmented_labels = []

        for data, label in zip(original_data, original_labels):
            perturbation_patterns = self.feature_learner.generate_perturbation_patterns(label)

            for j in range(n_augment_per_sample):
                # ç¬¬ä¸€æ­¥: è½»å¾®ç‰¹å¾æ‰°åŠ¨
                if perturbation_patterns:
                    pattern = perturbation_patterns[j % len(perturbation_patterns)]
                    pattern['intensity'] *= 0.2  # æ›´å°çš„æ‰°åŠ¨å¼ºåº¦
                    perturbed_data = self.apply_feature_based_perturbation(data, label, pattern)
                else:
                    perturbed_data = data + np.random.normal(0, 0.005, data.shape)  # å‡å°‘å™ªå£°

                # ç¬¬äºŒæ­¥: VAEå¢å¼º
                with torch.no_grad():
                    flat_data = perturbed_data.flatten().reshape(1, -1)
                    scaled_data = self.global_scaler.transform(flat_data)
                    tensor_data = torch.FloatTensor(scaled_data).to(self.device)

                    # åœ¨æ½œåœ¨ç©ºé—´æ·»åŠ é€‚åº¦å™ªå£°
                    mu, logvar = self.vae_gan.encode(tensor_data)
                    noise_scale = 0.01 + j * 0.005  # å‡å°‘å™ªå£°å¼ºåº¦
                    noise = torch.randn_like(mu) * noise_scale
                    z = mu + noise
                    enhanced_tensor = self.vae_gan.decode(z)

                    enhanced_scaled = enhanced_tensor.cpu().numpy()
                    enhanced_flat = self.global_scaler.inverse_transform(enhanced_scaled)
                    enhanced_data = enhanced_flat.reshape(data.shape)

                if self._is_valid_sample(enhanced_data, data):
                    augmented_data.append(enhanced_data)
                    augmented_labels.append(label)
                else:
                    # å¦‚æœè´¨é‡æ£€æŸ¥å¤±è´¥ï¼Œä½¿ç”¨æ›´ä¿å®ˆçš„æ–¹æ³•
                    fallback_data = data + np.random.normal(0, 0.005, data.shape)
                    augmented_data.append(fallback_data)
                    augmented_labels.append(label)

        print(f"âœ… æ··åˆå¢å¼ºå®Œæˆï¼Œç”Ÿæˆ {len(augmented_data)} ä¸ªæ ·æœ¬")
        return augmented_data, np.array(augmented_labels)

    def generate_augmented_samples(self, original_data, original_labels, augmentation_method='hybrid', n_augment_per_sample=2, **kwargs):
        """ç»Ÿä¸€çš„å¢å¼ºæ ·æœ¬ç”Ÿæˆæ¥å£

        Args:
            augmentation_method: å¢å¼ºæ–¹æ³•é€‰æ‹©
                - 'feature_perturbation': ä»…ç‰¹å¾æ‰°åŠ¨
                - 'vae_reconstruction': VAEé‡æ„å¢å¼º
                - 'hybrid': æ··åˆå¢å¼º
                - 'all': ä½¿ç”¨æ‰€æœ‰ä¸‰ç§æ–¹æ³•
            n_augment_per_sample: æ¯ä¸ªåŸå§‹æ ·æœ¬ç”Ÿæˆçš„å¢å¼ºæ ·æœ¬æ•°
            **kwargs: é¢å¤–å‚æ•°
        """
        print(f"ğŸ”„ å¼€å§‹æ•°æ®å¢å¼ºï¼Œæ–¹æ³•: {augmentation_method}")

        if augmentation_method == 'feature_perturbation':
            return self.generate_feature_perturbation_samples(original_data, original_labels, n_augment_per_sample)

        elif augmentation_method == 'vae_reconstruction':
            mix_ratio = kwargs.get('mix_ratio', 0.8)
            return self.generate_vae_reconstruction_samples(original_data, original_labels, n_augment_per_sample, mix_ratio)

        elif augmentation_method == 'hybrid':
            return self.generate_hybrid_samples(original_data, original_labels, n_augment_per_sample)

        elif augmentation_method == 'all':
            # ä½¿ç”¨æ‰€æœ‰ä¸‰ç§æ–¹æ³•ï¼Œæ¯ç§æ–¹æ³•ç”Ÿæˆ n_augment_per_sample // 3 ä¸ªæ ·æœ¬
            samples_per_method = max(1, n_augment_per_sample // 3)

            all_augmented_data = []
            all_augmented_labels = []

            # æ–¹æ¡ˆ1: ç‰¹å¾æ‰°åŠ¨
            aug_data_1, aug_labels_1 = self.generate_feature_perturbation_samples(
                original_data, original_labels, samples_per_method
            )
            all_augmented_data.extend(aug_data_1)
            all_augmented_labels.extend(aug_labels_1)

            # æ–¹æ¡ˆ2: VAEé‡æ„
            aug_data_2, aug_labels_2 = self.generate_vae_reconstruction_samples(
                original_data, original_labels, samples_per_method
            )
            all_augmented_data.extend(aug_data_2)
            all_augmented_labels.extend(aug_labels_2)

            # æ–¹æ¡ˆ3: æ··åˆå¢å¼º
            aug_data_3, aug_labels_3 = self.generate_hybrid_samples(
                original_data, original_labels, samples_per_method
            )
            all_augmented_data.extend(aug_data_3)
            all_augmented_labels.extend(aug_labels_3)

            print(f"âœ… æ‰€æœ‰æ–¹æ³•å¢å¼ºå®Œæˆï¼Œæ€»å…±ç”Ÿæˆ {len(all_augmented_data)} ä¸ªæ ·æœ¬")
            return all_augmented_data, np.array(all_augmented_labels)

        else:
            raise ValueError(f"æœªçŸ¥çš„å¢å¼ºæ–¹æ³•: {augmentation_method}")

    def _is_valid_sample(self, enhanced_data, original_data, threshold=10.0):
        """æ£€æŸ¥ç”Ÿæˆæ ·æœ¬çš„æœ‰æ•ˆæ€§ - è¿›ä¸€æ­¥æ”¾å®½æ¡ä»¶"""
        try:
            # 1. åŸºæœ¬æ•°å€¼æ£€æŸ¥
            if np.any(np.isnan(enhanced_data)) or np.any(np.isinf(enhanced_data)):
                return False

            # 2. æç«¯å€¼æ£€æŸ¥ - éå¸¸å®½æ¾
            if np.any(np.abs(enhanced_data) > 1e6):
                return False

            # 3. æ–¹å·®æ£€æŸ¥ - éå¸¸å®½æ¾
            enhanced_std = np.std(enhanced_data)
            if enhanced_std < 1e-6:  # å‡ ä¹ä¸º0çš„æ–¹å·®
                return False

            # å¯¹äºVAEé‡æ„ï¼Œæˆ‘ä»¬åº”è¯¥æ›´å®½æ¾
            return True

        except Exception as e:
            # å¦‚æœæ£€æŸ¥è¿‡ç¨‹å‡ºé”™ï¼Œé»˜è®¤æ¥å—æ ·æœ¬
            return True

    def compute_quality_score(self, sample, original_sample):
        """è®¡ç®—æ ·æœ¬è´¨é‡åˆ†æ•° - é’ˆå¯¹æ—¶åºæ•°æ®ä¼˜åŒ–çš„è¯„ä¼°æ ‡å‡†"""
        score = 0.0

        # å±•å¹³æ•°æ®ä»¥ä¾¿è®¡ç®—
        sample_flat = sample.flatten()
        original_flat = original_sample.flatten()

        # 1. åŸºæœ¬æ•°å€¼æœ‰æ•ˆæ€§æ£€æŸ¥ (0.1åˆ†)
        if not (np.isnan(sample_flat).any() or np.isinf(sample_flat).any()):
            score += 0.1
        else:
            return 0.0

        # 2. æ—¶åºç›¸å…³æ€§æ£€æŸ¥ (0.4åˆ†) - æœ€é‡è¦çš„æŒ‡æ ‡
        try:
            correlation = np.corrcoef(sample_flat, original_flat)[0, 1]
            if not np.isnan(correlation):
                if correlation > 0.8:  # é«˜ç›¸å…³æ€§
                    score += 0.4
                elif correlation > 0.6:  # ä¸­ç­‰ç›¸å…³æ€§
                    score += 0.25
                elif correlation > 0.4:  # ä½ç›¸å…³æ€§
                    score += 0.1
                else:  # ç›¸å…³æ€§å¤ªä½ï¼Œä¸¥é‡æƒ©ç½š
                    score *= 0.5
        except:
            score *= 0.5

        # 3. ç»Ÿè®¡ç‰¹æ€§ä¿æŒ (0.3åˆ†)
        sample_std = np.std(sample_flat)
        original_std = np.std(original_flat)
        sample_mean = np.mean(sample_flat)
        original_mean = np.mean(original_flat)

        if sample_std > 0 and original_std > 0:
            std_ratio = sample_std / original_std
            if 0.7 <= std_ratio <= 1.4:  # æ ‡å‡†å·®ä¿æŒåˆç†
                score += 0.15
            elif 0.5 <= std_ratio <= 2.0:
                score += 0.05

        if original_mean != 0:
            mean_ratio = abs(sample_mean / original_mean)
            if 0.8 <= mean_ratio <= 1.25:  # å‡å€¼ä¿æŒåˆç†
                score += 0.15
            elif 0.6 <= mean_ratio <= 1.67:
                score += 0.05

        # 4. åˆ†å¸ƒç›¸ä¼¼æ€§æ£€æŸ¥ (0.2åˆ†)
        try:
            _, p_value = stats.ks_2samp(sample_flat, original_flat)
            if p_value > 0.05:  # åˆ†å¸ƒç›¸ä¼¼
                score += 0.2
            elif p_value > 0.01:
                score += 0.1
        except:
            pass

        # 5. ä¸¥æ ¼çš„å¼‚å¸¸å€¼æƒ©ç½š
        q75, q25 = np.percentile(original_flat, [75, 25])
        iqr = q75 - q25
        if iqr > 0:
            outlier_threshold = 2 * iqr  # æ›´ä¸¥æ ¼çš„å¼‚å¸¸å€¼é˜ˆå€¼
            outliers = np.abs(sample_flat - np.median(original_flat)) > outlier_threshold
            outlier_ratio = np.sum(outliers) / len(sample_flat)
            if outlier_ratio > 0.05:  # è¶…è¿‡5%çš„å¼‚å¸¸å€¼å°±æƒ©ç½š
                score *= (1 - outlier_ratio * 2)  # æ›´ä¸¥å‰çš„æƒ©ç½š

        # 6. æ—¶åºæ¨¡å¼ä¿æŒæ£€æŸ¥ï¼ˆé’ˆå¯¹æ—¶åºæ•°æ®ï¼‰
        if len(sample.shape) > 1:  # å¦‚æœæ˜¯å¤šç»´æ—¶åºæ•°æ®
            try:
                # æ£€æŸ¥æ—¶åºè¶‹åŠ¿ä¿æŒ
                sample_2d = sample.reshape(-1, sample.shape[-1])
                original_2d = original_sample.reshape(-1, original_sample.shape[-1])

                # è®¡ç®—æ¯ä¸ªæ—¶é—´æ­¥çš„ç›¸å…³æ€§
                time_correlations = []
                for t in range(min(sample_2d.shape[1], original_2d.shape[1])):
                    if np.std(sample_2d[:, t]) > 0 and np.std(original_2d[:, t]) > 0:
                        corr = np.corrcoef(sample_2d[:, t], original_2d[:, t])[0, 1]
                        if not np.isnan(corr):
                            time_correlations.append(corr)

                if time_correlations:
                    avg_time_corr = np.mean(time_correlations)
                    if avg_time_corr < 0.3:  # æ—¶åºæ¨¡å¼ä¿æŒä¸å¥½
                        score *= 0.7
            except:
                pass

        return max(0.0, min(score, 1.0))

    def evaluate_augmentation_quality(self, original_data, original_labels, augmented_data, augmented_labels, config=None):
        """è¯„ä¼°å¢å¼ºè´¨é‡ - ä¿®å¤æ•°æ®æ³„éœ²é—®é¢˜"""
        print("ğŸ” è¯„ä¼°å¢å¼ºè´¨é‡...")

        # è®¡ç®—è´¨é‡åˆ†æ•° - ä¿®å¤ç´¢å¼•é€»è¾‘
        quality_scores = []
        n_augment_per_sample = len(augmented_data) // len(original_data) if len(original_data) > 0 else 2



        for i, aug_sample in enumerate(augmented_data):
            orig_idx = i // n_augment_per_sample  # åŠ¨æ€è®¡ç®—åŸå§‹æ ·æœ¬ç´¢å¼•
            if orig_idx < len(original_data):
                score = self.compute_quality_score(aug_sample, original_data[orig_idx])
                quality_scores.append(score)
            else:
                print(f"   âš ï¸ å¢å¼ºæ ·æœ¬ {i} å¯¹åº”çš„åŸå§‹æ ·æœ¬ç´¢å¼• {orig_idx} è¶…å‡ºèŒƒå›´")

        avg_quality = np.mean(quality_scores)
        print(f"   ğŸ“Š å¹³å‡è´¨é‡åˆ†æ•°: {avg_quality:.4f}")

        # åˆ†ç±»æ€§èƒ½è¯„ä¼° - å½»åº•ä¿®å¤æ•°æ®æ³„éœ²é—®é¢˜
        def flatten_data(data_list):
            return np.array([data.flatten() for data in data_list])

        X_orig = flatten_data(original_data)
        X_aug = flatten_data(augmented_data)
        y_all = np.hstack([original_labels, augmented_labels])

        # ä¿®å¤è¯„ä¼°é€»è¾‘ - ä½¿ç”¨äº¤å‰éªŒè¯è€Œéå›ºå®šåˆ†å‰²
        if len(original_labels) >= 6:
            # 1. æ£€æŸ¥ç±»åˆ«åˆ†å¸ƒ
            unique_labels, label_counts = np.unique(original_labels, return_counts=True)
            min_count = np.min(label_counts)

            print(f"   ğŸ“Š ç±»åˆ«åˆ†å¸ƒ: {dict(zip(unique_labels, label_counts))}")
            print(f"   ğŸ“Š æœ€å°‘ç±»åˆ«æ ·æœ¬æ•°: {min_count}")

            # 2. ä½¿ç”¨åˆ†å±‚KæŠ˜äº¤å‰éªŒè¯è€Œéå›ºå®šåˆ†å‰²
            from sklearn.model_selection import StratifiedKFold

            # æ ¹æ®æ•°æ®é‡é€‰æ‹©æŠ˜æ•°
            if len(original_labels) < 15:
                n_splits = 3  # å°æ•°æ®é›†ç”¨3æŠ˜
            elif len(original_labels) < 30:
                n_splits = 5  # ä¸­ç­‰æ•°æ®é›†ç”¨5æŠ˜
            else:
                n_splits = 10  # å¤§æ•°æ®é›†ç”¨10æŠ˜

            print(f"   ğŸ“Š ä½¿ç”¨ {n_splits} æŠ˜äº¤å‰éªŒè¯")

            # ä¸ºäº†ä¿æŒä»£ç å…¼å®¹æ€§ï¼Œæˆ‘ä»¬ä»ç„¶éœ€è¦è®­ç»ƒ/æµ‹è¯•åˆ†å‰²æ¥å¤„ç†å¢å¼ºæ•°æ®
            # ä½†ä¼šä½¿ç”¨äº¤å‰éªŒè¯æ¥è·å¾—æ›´å¯é çš„è¯„ä¼°
            if min_count >= 2:
                test_size = 0.2  # å‡å°‘æµ‹è¯•é›†ï¼Œå¢åŠ è®­ç»ƒé›†
            else:
                test_size = 1.0 / len(original_labels)

            X_train_raw, X_test_raw, y_train, y_test = train_test_split(
                X_orig, original_labels, test_size=test_size, random_state=42,
                stratify=original_labels
            )

            print(f"   ğŸ“Š è®­ç»ƒé›†å¤§å°: {len(X_train_raw)}, æµ‹è¯•é›†å¤§å°: {len(X_test_raw)}")

            # 2. ä»…åœ¨è®­ç»ƒé›†ä¸Šè®­ç»ƒé¢„å¤„ç†å™¨
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train_raw)
            X_test_scaled = scaler.transform(X_test_raw)  # åªå˜æ¢ï¼Œä¸è®­ç»ƒ

            # 3. è‡ªé€‚åº”PCAé™ç»´ - æ ¹æ®æ•°æ®ç‰¹æ€§è°ƒæ•´
            pca = None
            n_samples, n_features = X_train_scaled.shape

            if n_features > 50:
                # æ ¹æ®æ•°æ®ç±»å‹å’Œè§„æ¨¡è‡ªé€‚åº”é€‰æ‹©é™ç»´ç­–ç•¥
                if n_features > 10000:  # é«˜ç»´æ—¶åºæ•°æ®
                    # ä¿ç•™æ›´å¤šä¿¡æ¯ï¼Œä½¿ç”¨ç´¯ç§¯æ–¹å·®é˜ˆå€¼
                    pca_temp = PCA()
                    pca_temp.fit(X_train_scaled)
                    cumsum_var = np.cumsum(pca_temp.explained_variance_ratio_)

                    # ä¿®å¤ï¼šå¤§å¹…å¢åŠ ä¿ç•™çš„ç»´åº¦æ•°
                    n_components = np.argmax(cumsum_var >= 0.99) + 1  # ä¿ç•™99%æ–¹å·®
                    n_components = min(n_components, min(200, n_samples*3))  # å¤§å¹…å¢åŠ æœ€å¤§ç»´åº¦
                    n_components = max(n_components, min(50, n_samples))  # å¢åŠ æœ€å°ç»´åº¦

                elif n_features > 100:  # ä¸­ç­‰ç»´åº¦æ•°æ®
                    # ä¿®å¤ï¼šä¿ç•™90%æ–¹å·®ï¼Œå¢åŠ ä¿¡æ¯ä¿ç•™
                    pca_temp = PCA()
                    pca_temp.fit(X_train_scaled)
                    cumsum_var = np.cumsum(pca_temp.explained_variance_ratio_)

                    n_components = np.argmax(cumsum_var >= 0.90) + 1
                    n_components = min(n_components, n_samples//2, min(50, n_samples*2))
                    n_components = max(n_components, min(15, n_samples))

                else:  # ä½ç»´åº¦æ•°æ®
                    # ä¿å®ˆé™ç»´
                    n_components = min(20, n_samples//3, n_features)
                    n_components = max(n_components, 5)

                print(f"   ğŸ”§ è‡ªé€‚åº”PCAé™ç»´: {n_features} â†’ {n_components}")

                pca = PCA(n_components=n_components)
                X_train_scaled = pca.fit_transform(X_train_scaled)
                X_test_scaled = pca.transform(X_test_scaled)

                explained_ratio = np.sum(pca.explained_variance_ratio_)
                print(f"   ğŸ“Š PCAè§£é‡Šæ–¹å·®: {explained_ratio:.3f} ({explained_ratio*100:.1f}%)")



            # 4. å¤„ç†å¢å¼ºæ•°æ® - ä»…ä½¿ç”¨åŸºäºè®­ç»ƒé›†çš„å¢å¼ºæ•°æ®
            # æ‰¾å‡ºå“ªäº›å¢å¼ºæ ·æœ¬æ˜¯åŸºäºè®­ç»ƒé›†ç”Ÿæˆçš„
            train_indices_in_orig = []
            for i, (orig_data, orig_label) in enumerate(zip(X_orig, original_labels)):
                # æ£€æŸ¥è¿™ä¸ªåŸå§‹æ ·æœ¬æ˜¯å¦åœ¨è®­ç»ƒé›†ä¸­
                for train_data, train_label in zip(X_train_raw, y_train):
                    if np.array_equal(orig_data, train_data) and orig_label == train_label:
                        train_indices_in_orig.append(i)
                        break

            # è·å–åŸºäºè®­ç»ƒé›†çš„å¢å¼ºæ•°æ® - ä¿®å¤ç´¢å¼•é€»è¾‘
            X_aug_train = []
            y_aug_train = []
            n_augment_per_sample = len(augmented_data) // len(original_data) if len(original_data) > 0 else 2

            for i, (aug_data, aug_label) in enumerate(zip(X_aug, augmented_labels)):
                orig_idx = i // n_augment_per_sample  # åŠ¨æ€è®¡ç®—åŸå§‹æ ·æœ¬ç´¢å¼•
                if orig_idx < len(original_data) and orig_idx in train_indices_in_orig:
                    X_aug_train.append(aug_data)
                    y_aug_train.append(aug_label)

            # é¢„å¤„ç†å¢å¼ºæ•°æ®ï¼ˆä½¿ç”¨è®­ç»ƒé›†çš„é¢„å¤„ç†å™¨ï¼‰
            if X_aug_train:
                X_aug_train = np.array(X_aug_train)
                X_aug_train_scaled = scaler.transform(X_aug_train)
                if pca is not None:
                    X_aug_train_scaled = pca.transform(X_aug_train_scaled)
                y_aug_train = np.array(y_aug_train)
            else:
                X_aug_train_scaled = np.empty((0, X_train_scaled.shape[1]))
                y_aug_train = np.array([])

            # 5. åŸºçº¿æµ‹è¯•ï¼šä»…ä½¿ç”¨åŸå§‹è®­ç»ƒæ•°æ®
            clf_baseline = SVC(kernel='rbf', random_state=42)
            clf_baseline.fit(X_train_scaled, y_train)
            y_pred_baseline = clf_baseline.predict(X_test_scaled)
            acc_baseline = accuracy_score(y_test, y_pred_baseline)

            # 3. å¤šæ¨¡å‹äº¤å‰éªŒè¯è¯„ä¼° - é¿å…æ•°æ®æ³„éœ²
            print("   ğŸ”„ ä½¿ç”¨å¤šæ¨¡å‹äº¤å‰éªŒè¯è¯„ä¼°...")

            # è‡ªé€‚åº”æ¨¡å‹é…ç½® - æ ¹æ®æ•°æ®ç‰¹æ€§è°ƒæ•´å‚æ•°
            n_samples = len(X_train_scaled)
            n_features = X_train_scaled.shape[1]

            # ä¿®å¤ï¼šé‡æ–°è¯„ä¼°æ•°æ®å¤æ‚åº¦æŒ‡æ ‡ï¼Œè€ƒè™‘æå°æ•°æ®é›†
            sample_feature_ratio = n_samples / n_features

            # æ›´ä¸¥æ ¼çš„å¤æ‚åº¦è¯„ä¼°
            if sample_feature_ratio < 1.5:
                complexity_level = "extreme"  # æé«˜å¤æ‚åº¦
            elif sample_feature_ratio < 3:
                complexity_level = "high"
            elif sample_feature_ratio < 8:
                complexity_level = "medium"
            else:
                complexity_level = "low"

            print(f"   ğŸ“Š æ•°æ®å¤æ‚åº¦: {complexity_level} (æ ·æœ¬/ç‰¹å¾æ¯”={sample_feature_ratio:.2f})")

            if complexity_level == "extreme":  # æé«˜å¤æ‚åº¦ï¼šè½»åº¦æ­£åˆ™åŒ–
                classifiers = {
                    'SVM': SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42, probability=True),
                    'DecisionTree': DecisionTreeClassifier(
                        random_state=42, max_depth=5,  # è¿›ä¸€æ­¥å¢åŠ æ·±åº¦
                        min_samples_split=2,  # æœ€å°åˆ†å‰²è¦æ±‚
                        min_samples_leaf=1,   # æœ€å°å¶å­è¦æ±‚
                        max_features=None, ccp_alpha=0.0  # æ— å‰ªæ
                    ),
                    'LogisticRegression': LogisticRegression(
                        random_state=42, max_iter=1000, C=1.0,  # æ ‡å‡†æ­£åˆ™åŒ–
                        penalty='l2'
                    ),
                    'KNN': KNeighborsClassifier(
                        n_neighbors=min(5, max(3, n_samples//4)),
                        weights='distance'
                    ),
                    'NaiveBayes': GaussianNB(),
                }
            elif complexity_level == "high":  # é«˜å¤æ‚åº¦ï¼šå¼ºæ­£åˆ™åŒ–
                classifiers = {
                    'SVM': SVC(kernel='linear', C=0.01, random_state=42, probability=True),
                    'DecisionTree': DecisionTreeClassifier(
                        random_state=42, max_depth=2,
                        min_samples_split=max(5, n_samples//2),
                        min_samples_leaf=max(3, n_samples//5),
                        max_features=min(3, n_features), ccp_alpha=0.05
                    ),
                    'LogisticRegression': LogisticRegression(
                        random_state=42, max_iter=1000, C=0.01,
                        penalty='l1', solver='liblinear'
                    ),
                    'KNN': KNeighborsClassifier(
                        n_neighbors=min(n_samples-1, max(5, n_samples//2)),
                        weights='distance'
                    ),
                    'NaiveBayes': GaussianNB(),
                }
            elif complexity_level == "medium":  # ä¸­ç­‰å¤æ‚åº¦ï¼šé€‚ä¸­æ­£åˆ™åŒ–
                classifiers = {
                    'SVM': SVC(kernel='rbf', C=0.1, random_state=42, probability=True),
                    'DecisionTree': DecisionTreeClassifier(
                        random_state=42, max_depth=4,
                        min_samples_split=max(3, n_samples//4),
                        min_samples_leaf=max(2, n_samples//8),
                        max_features='sqrt', ccp_alpha=0.02
                    ),
                    'LogisticRegression': LogisticRegression(
                        random_state=42, max_iter=1000, C=0.1,
                        penalty='l2'
                    ),
                    'KNN': KNeighborsClassifier(
                        n_neighbors=min(7, max(3, n_samples//4)),
                        weights='distance'
                    ),
                    'NaiveBayes': GaussianNB(),
                }
            else:  # ä½å¤æ‚åº¦ï¼šè½»åº¦æ­£åˆ™åŒ–
                classifiers = {
                    'SVM': SVC(kernel='rbf', C=1.0, random_state=42, probability=True),
                    'DecisionTree': DecisionTreeClassifier(
                        random_state=42, max_depth=6,
                        min_samples_split=max(2, n_samples//6),
                        min_samples_leaf=max(1, n_samples//12),
                        max_features='sqrt', ccp_alpha=0.01
                    ),
                    'LogisticRegression': LogisticRegression(
                        random_state=42, max_iter=1000, C=1.0,
                        penalty='l2'
                    ),
                    'KNN': KNeighborsClassifier(
                        n_neighbors=min(5, max(3, n_samples//5)),
                        weights='uniform'
                    ),
                    'NaiveBayes': GaussianNB(),
                }

            # è‡ªé€‚åº”æ·±åº¦å­¦ä¹ æ¨¡å‹
            if torch.cuda.is_available():
                # æ ¹æ®å¤æ‚åº¦è°ƒæ•´æ·±åº¦å­¦ä¹ å‚æ•°
                if complexity_level == "extreme":
                    epochs, lr = 3, 0.05
                elif complexity_level == "high":
                    epochs, lr = 5, 0.01
                elif complexity_level == "medium":
                    epochs, lr = 10, 0.005
                else:
                    epochs, lr = 15, 0.001

                classifiers['CNN'] = DeepClassifierWrapper(
                    model_type='cnn',
                    input_dim=X_train_scaled.shape[1],
                    epochs=epochs,
                    learning_rate=lr,
                    device=self.device
                )
                classifiers['LSTM'] = DeepClassifierWrapper(
                    model_type='lstm',
                    input_dim=X_train_scaled.shape[1],
                    epochs=epochs,
                    learning_rate=lr,
                    device=self.device
                )

            # è‡ªé€‚åº”åŸå‹ç½‘ç»œ
            if config is None:
                config = {}
            protonet_enabled = config.get('protonet_enabled', True)
            if torch.cuda.is_available() and protonet_enabled:
                # æ ¹æ®å¤æ‚åº¦è°ƒæ•´åŸå‹ç½‘ç»œå‚æ•°
                if complexity_level == "extreme":
                    hidden_dim, z_dim, dropout, epochs = 4, 2, 0.7, 3  # æç®€é…ç½®
                elif complexity_level == "high":
                    hidden_dim, z_dim, dropout, epochs = 8, 4, 0.5, 5
                elif complexity_level == "medium":
                    hidden_dim, z_dim, dropout, epochs = 16, 8, 0.4, 10
                else:
                    hidden_dim, z_dim, dropout, epochs = 32, 16, 0.3, 15

                classifiers['ProtoNet'] = ProtoNetClassifier(
                    input_dim=X_train_scaled.shape[1],
                    hidden_dim=hidden_dim,
                    z_dim=z_dim,
                    dropout_rate=dropout,
                    epochs=epochs,
                    learning_rate=config.get('protonet_lr', 0.01),
                    temperature=config.get('protonet_temperature', 3.0),
                    device=self.device,
                    auto_adapt=True
                )


            # 7. æ”¹è¿›çš„è¯„ä¼°æ–¹æ³• - ä½¿ç”¨äº¤å‰éªŒè¯
            print("   ğŸ“Š ä½¿ç”¨äº¤å‰éªŒè¯è¯„ä¼°æ¨¡å‹æ€§èƒ½...")

            # ä¸ºäº¤å‰éªŒè¯å‡†å¤‡å®Œæ•´æ•°æ®é›†
            scaler_full = StandardScaler()
            X_orig_scaled = scaler_full.fit_transform(X_orig)

            # å¦‚æœéœ€è¦PCAï¼Œä¹Ÿåœ¨å®Œæ•´æ•°æ®é›†ä¸Šåº”ç”¨
            if pca is not None:
                pca_full = PCA(n_components=pca.n_components_)
                X_orig_scaled = pca_full.fit_transform(X_orig_scaled)

            baseline_results = {}
            augmented_results = {}

            # åŸºçº¿è¯„ä¼°ï¼šä½¿ç”¨äº¤å‰éªŒè¯
            skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)

            for name, clf in classifiers.items():
                try:
                    if name in ['CNN', 'LSTM', 'ProtoNet']:
                        # æ·±åº¦å­¦ä¹ æ¨¡å‹ä½¿ç”¨ç®€å•åˆ†å‰²ï¼ˆäº¤å‰éªŒè¯å¤ªè€—æ—¶ï¼‰
                        clf.fit(X_train_scaled, y_train)
                        y_pred = clf.predict(X_test_scaled)
                        baseline_results[name] = accuracy_score(y_test, y_pred)
                    else:
                        # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ä½¿ç”¨äº¤å‰éªŒè¯
                        cv_scores = cross_val_score(clf, X_orig_scaled, original_labels,
                                                  cv=skf, scoring='accuracy')
                        baseline_results[name] = np.mean(cv_scores)
                except Exception:
                    baseline_results[name] = 0.0

            # å¢å¼ºæ•°æ®è¯„ä¼°ï¼šä½¿ç”¨è®­ç»ƒé›†+å¢å¼ºæ•°æ®è®­ç»ƒï¼Œåœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
            if len(X_aug_train_scaled) > 0:
                X_combined = np.vstack([X_train_scaled, X_aug_train_scaled])
                y_combined = np.hstack([y_train, y_aug_train])

                for name, clf in classifiers.items():
                    try:
                        if name in ['CNN', 'LSTM', 'ProtoNet']:
                            # é‡æ–°åˆå§‹åŒ–è‡ªé€‚åº”æ·±åº¦å­¦ä¹ æ¨¡å‹
                            if name == 'CNN':
                                clf = DeepClassifierWrapper('cnn', X_train_scaled.shape[1],
                                                          epochs=epochs, learning_rate=lr, device=self.device)
                            elif name == 'LSTM':
                                clf = DeepClassifierWrapper('lstm', X_train_scaled.shape[1],
                                                          epochs=epochs, learning_rate=lr, device=self.device)
                            elif name == 'ProtoNet':
                                clf = ProtoNetClassifier(
                                    input_dim=X_train_scaled.shape[1],
                                    hidden_dim=hidden_dim,
                                    z_dim=z_dim,
                                    dropout_rate=dropout,
                                    epochs=epochs,
                                    learning_rate=config.get('protonet_lr', 0.01),
                                    temperature=config.get('protonet_temperature', 3.0),
                                    device=self.device,
                                    auto_adapt=True
                                )

                            clf.fit(X_combined, y_combined)
                            y_pred = clf.predict(X_test_scaled)
                            augmented_results[name] = accuracy_score(y_test, y_pred)
                        else:
                            # ä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹ - é‡æ–°åˆå§‹åŒ–é˜²è¿‡æ‹Ÿåˆç‰ˆæœ¬
                            n_combined = len(X_combined)
                            if name == 'SVM':
                                clf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42, probability=True)
                            elif name == 'DecisionTree':
                                clf = DecisionTreeClassifier(
                                    random_state=42, max_depth=3,
                                    min_samples_split=max(2, n_combined//5),
                                    min_samples_leaf=max(1, n_combined//10),
                                    max_features='sqrt', ccp_alpha=0.01
                                )
                            elif name == 'LogisticRegression':
                                clf = LogisticRegression(
                                    random_state=42, max_iter=1000, C=0.1,
                                    penalty='l1', solver='liblinear'
                                )
                            elif name == 'KNN':
                                clf = KNeighborsClassifier(
                                    n_neighbors=min(n_combined-1, max(5, n_combined//2)),
                                    weights='distance'
                                )
                            elif name == 'NaiveBayes':
                                clf = GaussianNB()

                            clf.fit(X_combined, y_combined)
                            y_pred = clf.predict(X_test_scaled)
                            augmented_results[name] = accuracy_score(y_test, y_pred)
                    except Exception:
                        augmented_results[name] = baseline_results.get(name, 0.0)
            else:
                augmented_results = baseline_results.copy()



            # ä½¿ç”¨SVMä½œä¸ºä¸»è¦è¯„ä¼°æŒ‡æ ‡ï¼ˆé€‚åˆè®¤çŸ¥è¡°å¼±åˆ†ç±»ï¼‰
            acc_baseline = baseline_results.get('SVM', 0.0)
            acc_augmented = augmented_results.get('SVM', 0.0)



            # ä½¿ç”¨SVMä½œä¸ºä¸»è¦è¯„ä¼°æŒ‡æ ‡

            # æ˜¾ç¤ºæ‰€æœ‰æ¨¡å‹çš„ç»“æœ
            print(f"   ğŸ“Š å¤šæ¨¡å‹è¯„ä¼°ç»“æœ:")
            print(f"   {'æ¨¡å‹':<12} {'åŸºçº¿å‡†ç¡®ç‡':<12} {'å¢å¼ºå‡†ç¡®ç‡':<12} {'æ€§èƒ½æå‡':<12}")
            print("   " + "-" * 50)

            for name in classifiers.keys():
                baseline_acc = baseline_results.get(name, 0.0)
                augmented_acc = augmented_results.get(name, 0.0)
                improvement = augmented_acc - baseline_acc
                print(f"   {name:<12} {baseline_acc:<12.4f} {augmented_acc:<12.4f} {improvement:<12.4f}")

            print(f"\n   ğŸ“Š ä¸»è¦æŒ‡æ ‡ (SVM):")
            print(f"   ğŸ“Š åŸºçº¿å‡†ç¡®ç‡: {acc_baseline:.4f}")
            print(f"   ğŸ“Š å¢å¼ºå‡†ç¡®ç‡: {acc_augmented:.4f}")
            print(f"   ğŸ“ˆ æ€§èƒ½æå‡: {acc_augmented - acc_baseline:.4f}")

            # å¦‚æœæœ‰åŸå‹ç½‘ç»œç»“æœï¼Œä¹Ÿæ˜¾ç¤º
            if 'ProtoNet' in baseline_results:
                proto_baseline = baseline_results['ProtoNet']
                proto_augmented = augmented_results['ProtoNet']
                proto_improvement = proto_augmented - proto_baseline
                print(f"\n   ğŸ§  åŸå‹ç½‘ç»œç»“æœ:")
                print(f"   ğŸ“Š åŸºçº¿å‡†ç¡®ç‡: {proto_baseline:.4f}")
                print(f"   ğŸ“Š å¢å¼ºå‡†ç¡®ç‡: {proto_augmented:.4f}")
                print(f"   ğŸ“ˆ æ€§èƒ½æå‡: {proto_improvement:.4f}")

            # æ„å»ºè¯¦ç»†çš„ç»“æœå­—å…¸
            result_dict = {
                'quality_score': avg_quality,
                'baseline_accuracy': acc_baseline,
                'augmented_accuracy': acc_augmented,
                'improvement': acc_augmented - acc_baseline,
                'all_models': {
                    'baseline_results': baseline_results,
                    'augmented_results': augmented_results,
                    'improvements': {name: augmented_results[name] - baseline_results[name]
                                   for name in baseline_results.keys()}
                }
            }

            return result_dict
        else:
            print("   âš ï¸ æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡åˆ†ç±»è¯„ä¼°")
            return {'quality_score': avg_quality}

    def save_augmented_data(self, augmented_data, augmented_labels, output_dir):
        """ä¿å­˜å¢å¼ºæ•°æ®ä¸ºå•ç‹¬çš„CSVæ–‡ä»¶"""
        print(f"ğŸ’¾ ä¿å­˜å¢å¼ºæ•°æ®åˆ° {output_dir}...")

        os.makedirs(output_dir, exist_ok=True)

        for i, (data, label) in enumerate(zip(augmented_data, augmented_labels)):
            # åˆ›å»ºDataFrame
            df = pd.DataFrame(data)
            df['label'] = label

            # ä¿å­˜ä¸ºCSVæ–‡ä»¶
            filename = f"augmented_data_{i:04d}.csv"
            filepath = os.path.join(output_dir, filename)
            df.to_csv(filepath, index=False)

        print(f"âœ… ä¿å­˜å®Œæˆï¼Œå…± {len(augmented_data)} ä¸ªæ–‡ä»¶")

        # ä¿å­˜è´¨é‡è¯„ä¼°æŠ¥å‘Š
        return output_dir

def run_feature_based_augmentation(config):
    """è¿è¡ŒåŸºäºç‰¹å¾å­¦ä¹ çš„æ•°æ®å¢å¼º"""

    print("ğŸ§  åŸºäºç‰¹å¾å­¦ä¹ çš„è®¤çŸ¥è¡°å¼±æ•°æ®å¢å¼ºç³»ç»Ÿ")
    print("=" * 60)

    # åˆå§‹åŒ–ç³»ç»Ÿ
    aug_system = FeatureBasedAugmentationSystem(
        seq_len=config['seq_len'],
        latent_dim=config['latent_dim'],
        device=config['device']
    )

    try:
        # 1. åŠ è½½å’Œå¤„ç†æ•°æ®
        original_data, original_labels, data_by_class = aug_system.load_and_process_data(
            config['data_path']
        )

        # 2. å­¦ä¹ å„ç±»åˆ«ç‰¹å¾
        aug_system.feature_learner.learn_class_features(data_by_class)

        # 3. è®­ç»ƒVAE-GANç½‘ç»œ
        aug_system.train_vae_gan(
            original_data,
            original_labels,
            epochs=config['vae_epochs'],
            batch_size=config['batch_size']
        )

        # 4. ç”Ÿæˆå¢å¼ºæ ·æœ¬
        augmented_data, augmented_labels = aug_system.generate_augmented_samples(
            original_data,
            original_labels,
            augmentation_method=config['augmentation_method'],
            n_augment_per_sample=config['n_augment_per_sample'],
            mix_ratio=config.get('mix_ratio', 0.8)
        )

        # 5. è¯„ä¼°å¢å¼ºè´¨é‡
        quality_metrics = aug_system.evaluate_augmentation_quality(
            original_data, original_labels,
            augmented_data, augmented_labels,
            config
        )

        # 6. ä¿å­˜å¢å¼ºæ•°æ®
        output_dir = aug_system.save_augmented_data(
            augmented_data, augmented_labels, config['output_dir']
        )

        # 7. ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
        if create_comprehensive_report is not None:
            create_comprehensive_report(
                original_data, augmented_data, original_labels, augmented_labels,
                quality_metrics, output_dir
            )

        # 8. ä¿å­˜è´¨é‡è¯„ä¼°æŠ¥å‘Š
        import json
        with open(os.path.join(output_dir, 'quality_report.json'), 'w', encoding='utf-8') as f:
            json.dump(quality_metrics, f, indent=2, ensure_ascii=False)

        # 9. æ‰“å°æ€»ç»“
        print("\n" + "=" * 60)
        print("ğŸ“Š å¢å¼ºå®Œæˆæ€»ç»“")
        print("=" * 60)
        print(f"ğŸ“‚ åŸå§‹æ•°æ®: {len(original_data)} ä¸ªæ ·æœ¬")
        print(f"ğŸ“‚ å¢å¼ºæ•°æ®: {len(augmented_data)} ä¸ªæ ·æœ¬")
        print(f"ğŸ“ˆ æ•°æ®å¢é•¿: {len(augmented_data) / len(original_data):.1f}x")
        print(f"ğŸ“Š è´¨é‡åˆ†æ•°: {quality_metrics.get('quality_score', 0):.4f}")

        if 'baseline_accuracy' in quality_metrics:
            print(f"ğŸ“Š åŸºçº¿å‡†ç¡®ç‡: {quality_metrics['baseline_accuracy']:.4f}")
            print(f"ğŸ“Š å¢å¼ºå‡†ç¡®ç‡: {quality_metrics['augmented_accuracy']:.4f}")
            print(f"ğŸ“ˆ æ€§èƒ½æå‡: {quality_metrics['improvement']:.4f}")

        print(f"ğŸ’¾ ç»“æœä¿å­˜åœ¨: {output_dir}")

        return quality_metrics

    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

def main():
    """ä¸»å‡½æ•° - é…ç½®å‚æ•°å¹¶è¿è¡Œå¢å¼ºç³»ç»Ÿ"""

    # ==================== é…ç½®å‚æ•° ====================
    config = {
        # æ•°æ®è·¯å¾„é…ç½®
        'data_path': './feature1',  # åŸå§‹æ•°æ®ç›®å½•
        'output_dir': 'augmented_results',  # è¾“å‡ºç›®å½•

        # æ•°æ®å¤„ç†å‚æ•°
        'seq_len': 400,  # åºåˆ—é•¿åº¦

        # VAE-GANç½‘ç»œå‚æ•°
        'latent_dim': 64,  # æ½œåœ¨ç©ºé—´ç»´åº¦
        'vae_epochs': 1000,  # VAE-GANè®­ç»ƒè½®æ•°
        'batch_size': 16,  # æ‰¹æ¬¡å¤§å°
        'device': 'cuda',  # è®¡ç®—è®¾å¤‡

        # å¢å¼ºæ–¹æ³•é€‰æ‹© (é‡è¦å‚æ•°)
        'augmentation_method': 'hybrid',  # å¯é€‰: 'feature_perturbation', 'vae_reconstruction', 'hybrid', 'all'

        # å¢å¼ºå‚æ•°
        'n_augment_per_sample': 2,  # æ¯ä¸ªåŸå§‹æ ·æœ¬ç”Ÿæˆçš„å¢å¼ºæ ·æœ¬æ•°
        'mix_ratio': 0.8,  # VAEé‡æ„æ—¶ä¸åŸå§‹æ•°æ®çš„æ··åˆæ¯”ä¾‹ (ä»…å¯¹vae_reconstructionæ–¹æ³•æœ‰æ•ˆ)

        # ç‰¹å¾å­¦ä¹ å‚æ•°
        'feature_components': 20,  # PCAç‰¹å¾æ•°é‡
    }
    # ================================================

    print("ğŸ“‹ é…ç½®ä¿¡æ¯:")
    for key, value in config.items():
        print(f"   {key}: {value}")
    print()

    # æ£€æŸ¥æ•°æ®ç›®å½•
    if not os.path.exists(config['data_path']):
        print(f"âŒ é”™è¯¯: æ•°æ®ç›®å½•ä¸å­˜åœ¨: {config['data_path']}")
        print("ğŸ’¡ è¯·ç¡®ä¿originalç›®å½•å­˜åœ¨å¹¶åŒ…å«CSVæ–‡ä»¶")
        return

    # è¿è¡Œå¢å¼ºç³»ç»Ÿ
    results = run_feature_based_augmentation(config)

    if results:
        print("\nğŸ‰ å¢å¼ºç³»ç»Ÿè¿è¡ŒæˆåŠŸ!")
    else:
        print("\nâŒ å¢å¼ºç³»ç»Ÿè¿è¡Œå¤±è´¥!")

if __name__ == "__main__":
    main()
