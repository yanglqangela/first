#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è®¤çŸ¥éšœç¢æ•°æ®å¢å¼ºç³»ç»Ÿ - ä¸»è¿è¡Œç¨‹åº
ä»CSVæ–‡ä»¶åŠ è½½åŸå§‹æ•°æ®ï¼Œåº”ç”¨å››ç§è®¤çŸ¥è¡°é€€æ¨¡æ‹Ÿæ–¹æ³•è¿›è¡Œæ•°æ®å¢å¼º
åŒ…å«åŸºäºç±»åˆ«çš„å·®å¼‚åŒ–å¢å¼ºå’Œè´¨é‡è¯„ä¼°åŠŸèƒ½
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import os
import sys
from pathlib import Path
import warnings

# Set matplotlib to avoid font warnings
plt.rcParams['font.family'] = ['DejaVu Sans', 'sans-serif']
plt.rcParams['axes.unicode_minus'] = False
warnings.filterwarnings('ignore', category=UserWarning, module='matplotlib')

# Add path
current_dir = Path(__file__).parent
sys.path.insert(0, str(current_dir))

from cognitive_augmentation import CognitiveAugmentationSystem
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, f1_score
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV
import seaborn as sns
from scipy import stats

def load_data(data_dir, seq_len=400):
    """ä»ç›®å½•åŠ è½½å¤šä¸ªCSVæ–‡ä»¶"""
    print(f"ğŸ“‚ ä» {data_dir} åŠ è½½æ•°æ®...")

    if not os.path.exists(data_dir):
        raise ValueError(f"ç›®å½• '{data_dir}' ä¸å­˜åœ¨")

    # æŸ¥æ‰¾æ‰€æœ‰CSVæ–‡ä»¶
    data_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]
    if not data_files:
        raise ValueError(f"åœ¨ {data_dir} ä¸­æœªæ‰¾åˆ°CSVæ–‡ä»¶")

    print(f"ğŸ“Š æ‰¾åˆ° {len(data_files)} ä¸ªæ•°æ®æ–‡ä»¶")
    
    all_data = []
    all_labels = []
    
    for file in data_files:
        try:
            df = pd.read_csv(os.path.join(data_dir, file))

            # æå–æ ‡ç­¾
            if 'label' in df.columns:
                label = int(df['label'].iloc[0])
                df = df.drop('label', axis=1)
            else:
                # ä»æ–‡ä»¶åæå–æ ‡ç­¾
                import re
                numbers = re.findall(r'\d+', file)
                label = int(numbers[0]) % 3 if numbers else 0

            # æ•°æ®é¢„å¤„ç†
            data = df.values.astype(np.float32)
            data = np.nan_to_num(data, nan=0.0, posinf=1.0, neginf=-1.0)

            # æˆªæ–­æˆ–å¡«å……åˆ°æŒ‡å®šé•¿åº¦
            if len(data) > seq_len:
                data = data[:seq_len]
            else:
                padding = np.zeros((seq_len - len(data), data.shape[1]))
                data = np.vstack([data, padding])

            all_data.append(data)
            all_labels.append(label)

        except Exception as e:
            print(f"âš ï¸ å¤„ç†æ–‡ä»¶ {file} æ—¶å‡ºé”™: {e}")
            continue
    
    if not all_data:
        raise ValueError("æ²¡æœ‰æˆåŠŸåŠ è½½ä»»ä½•æ•°æ®æ–‡ä»¶")

    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆ")
    print(f"ğŸ“Š æ ·æœ¬æ•°é‡: {len(all_data)}")
    print(f"ğŸ“Š æ¯ä¸ªæ ·æœ¬å½¢çŠ¶: {all_data[0].shape}")
    print(f"ğŸ·ï¸ æ ‡ç­¾åˆ†å¸ƒ: {np.bincount(all_labels)}")

    return all_data, np.array(all_labels)

def visualize_results(original_data, augmented_results, output_dir):
    """Visualize augmentation results"""
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 8))
    fig.suptitle('Cognitive Augmentation Results', fontsize=14)
    
    # Original data
    axes[0, 0].plot(original_data[:, :6], alpha=0.7, linewidth=1)
    axes[0, 0].set_title('Original')
    axes[0, 0].grid(True, alpha=0.3)
    
    # Augmentation results
    method_names = ['Motor Delay', 'Gait Perturbation', 'Sensor Drift', 'Cognitive Vector']
    positions = [(0, 1), (0, 2), (1, 0), (1, 1)]
    
    for i, (method_key, aug_data) in enumerate(augmented_results.items()):
        if i < len(positions):
            row, col = positions[i]
            axes[row, col].plot(aug_data[:, :6], alpha=0.7, linewidth=1)
            axes[row, col].set_title(method_names[i])
            axes[row, col].grid(True, alpha=0.3)
    
    # Change magnitude comparison
    changes = [np.linalg.norm(original_data - aug_data) for aug_data in augmented_results.values()]
    axes[1, 2].bar(range(len(changes)), changes, alpha=0.7)
    axes[1, 2].set_title('Change Magnitude')
    axes[1, 2].set_xticks(range(len(changes)))
    axes[1, 2].set_xticklabels(['MD', 'GP', 'SD', 'CV'], rotation=0)
    axes[1, 2].grid(True, alpha=0.3)
    
    plt.tight_layout()
    plt.savefig(os.path.join(output_dir, 'augmentation_comparison.png'), 
                dpi=300, bbox_inches='tight')
    plt.close()

def get_class_specific_params(class_label, base_params, intensity_factors):
    """æ ¹æ®è®¤çŸ¥çŠ¶æ€è·å–ç±»åˆ«ç‰¹å®šçš„å¢å¼ºå‚æ•°"""

    # ç±»åˆ«0: æ­£å¸¸äºº - æœ€å°å¢å¼º
    # ç±»åˆ«1: è½»åº¦è®¤çŸ¥éšœç¢ - ä¸­ç­‰å¢å¼º
    # ç±»åˆ«2: ä¸­åº¦è®¤çŸ¥è¡°é€€ - å¼ºçƒˆå¢å¼º

    if class_label not in intensity_factors:
        class_label = 1  # é»˜è®¤ä½¿ç”¨è½»åº¦è®¤çŸ¥éšœç¢å‚æ•°

    factors = intensity_factors[class_label]
    class_params = {}

    # è¿åŠ¨å»¶è¿Ÿå‚æ•°è°ƒæ•´
    base_delay = base_params['motor_delay']['delay_range']
    base_alpha = base_params['motor_delay']['alpha_range']

    delay_factor = factors['delay_factor']
    new_delay_range = [
        max(1, int(base_delay[0] * delay_factor)),
        min(15, int(base_delay[1] * delay_factor))
    ]
    new_alpha_range = [
        max(0.5, base_alpha[0] * (2 - delay_factor)),  # åå‘è°ƒæ•´
        min(0.99, base_alpha[1] * (2 - delay_factor))
    ]

    class_params['motor_delay'] = {
        'delay_range': new_delay_range,
        'alpha_range': new_alpha_range
    }

    # æ­¥æ€æ‰°åŠ¨å‚æ•°è°ƒæ•´
    base_perturbation = base_params['gait_perturbation']['perturbation_intensity']
    base_sync = base_params['gait_perturbation']['sync_probability']

    class_params['gait_perturbation'] = {
        'perturbation_intensity': min(0.8, base_perturbation * factors['perturbation_factor']),
        'sync_probability': min(0.8, base_sync * factors['perturbation_factor'])
    }

    # ä¼ æ„Ÿå™¨æ¼‚ç§»å‚æ•°è°ƒæ•´
    base_drift = base_params['sensor_drift']['drift_intensity']

    class_params['sensor_drift'] = {
        'drift_intensity': min(0.2, base_drift * factors['drift_factor'])
    }

    return class_params

def compute_quality_score(sample, original_data=None):
    """è®¡ç®—æ ·æœ¬è´¨é‡åˆ†æ•° - å‚è€ƒenhanced_vae_gan.py"""
    score = 0.0

    # 1. ç»Ÿè®¡ç‰¹æ€§æ£€æŸ¥
    if not (np.isnan(sample).any() or np.isinf(sample).any()):
        score += 0.2

    # 2. æ–¹å·®æ£€æŸ¥ï¼ˆé¿å…å¸¸æ•°åºåˆ—ï¼‰
    if np.std(sample) > 0.01:
        score += 0.2

    # 3. èŒƒå›´æ£€æŸ¥
    sample_min, sample_max = sample.min(), sample.max()
    if sample_min >= -5 and sample_max <= 5:  # åˆç†èŒƒå›´
        score += 0.2

    # 4. å¹³æ»‘æ€§æ£€æŸ¥ï¼ˆé¿å…è¿‡åº¦éœ‡è¡ï¼‰
    if len(sample) > 1:
        diff = np.diff(sample)
        if np.std(diff) < 2.0 * np.std(sample):
            score += 0.2

    # 5. å¦‚æœæœ‰åŸå§‹æ•°æ®ï¼Œæ£€æŸ¥åˆ†å¸ƒç›¸ä¼¼æ€§
    if original_data is not None:
        try:
            # KSæµ‹è¯•
            _, p_value = stats.ks_2samp(sample, original_data.flatten())
            if p_value > 0.05:  # åˆ†å¸ƒç›¸ä¼¼
                score += 0.2
        except:
            pass
    else:
        score += 0.2  # é»˜è®¤ç»™åˆ†

    return score

def train_and_evaluate_classifiers(train_X, train_y, test_X, test_y, classifier_params, experiment_name=""):
    """è®­ç»ƒå’Œè¯„ä¼°å¤šä¸ªåˆ†ç±»å™¨ - ä½¿ç”¨æ›´å¤æ‚çš„æ¨¡å‹"""

    # ä½¿ç”¨ä¼ å…¥çš„åˆ†ç±»å™¨é…ç½®
    classifiers = {
        'Random Forest': RandomForestClassifier(
            random_state=42,
            **classifier_params['random_forest']
        ),
        'Gradient Boosting': GradientBoostingClassifier(
            random_state=42,
            **classifier_params['gradient_boosting']
        ),
        'SVM': SVC(
            random_state=42,
            **classifier_params['svm']
        ),
        'MLP': MLPClassifier(
            random_state=42,
            **classifier_params['mlp']
        )
    }

    results = {}

    for name, clf in classifiers.items():
        print(f"   ğŸ”§ è®­ç»ƒ {name}...")

        try:
            # è®­ç»ƒ
            clf.fit(train_X, train_y)

            # é¢„æµ‹
            pred_y = clf.predict(test_X)

            # è¯„ä¼°
            accuracy = accuracy_score(test_y, pred_y)
            f1 = f1_score(test_y, pred_y, average='weighted')

            results[name] = {
                'accuracy': accuracy,
                'f1_score': f1,
                'predictions': pred_y,
                'true_labels': test_y
            }

            print(f"     ğŸ“Š å‡†ç¡®ç‡: {accuracy:.4f}")
            print(f"     ğŸ“Š F1åˆ†æ•°: {f1:.4f}")

        except Exception as e:
            print(f"     âŒ {name} è®­ç»ƒå¤±è´¥: {e}")
            results[name] = {
                'accuracy': 0.0,
                'f1_score': 0.0,
                'predictions': np.zeros_like(test_y),
                'true_labels': test_y
            }

    return results

def cross_validation_comparison(orig_X, orig_y, aug_X, aug_y, cv_folds=5):
    """äº¤å‰éªŒè¯æ¯”è¾ƒ"""

    print("   ğŸ”„ æ‰§è¡Œäº¤å‰éªŒè¯...")

    # ä»…åŸå§‹æ•°æ®
    rf_orig = RandomForestClassifier(n_estimators=100, random_state=42)
    scores_orig = cross_val_score(rf_orig, orig_X, orig_y,
                                 cv=StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42),
                                 scoring='accuracy')

    # åŸå§‹+å¢å¼ºæ•°æ®
    combined_X = np.vstack([orig_X, aug_X])
    combined_y = np.hstack([orig_y, aug_y])

    rf_aug = RandomForestClassifier(n_estimators=100, random_state=42)
    scores_aug = cross_val_score(rf_aug, combined_X, combined_y,
                                cv=StratifiedKFold(n_splits=cv_folds, shuffle=True, random_state=42),
                                scoring='accuracy')

    print(f"   ğŸ“Š åŸå§‹æ•°æ®CVå‡†ç¡®ç‡: {scores_orig.mean():.4f} Â± {scores_orig.std():.4f}")
    print(f"   ğŸ“Š å¢å¼ºæ•°æ®CVå‡†ç¡®ç‡: {scores_aug.mean():.4f} Â± {scores_aug.std():.4f}")
    print(f"   ğŸ“ˆ æ€§èƒ½æå‡: {(scores_aug.mean() - scores_orig.mean()):.4f}")

    return {
        'original_scores': scores_orig,
        'augmented_scores': scores_aug,
        'improvement': scores_aug.mean() - scores_orig.mean()
    }

def evaluate_augmentation_quality(original_data, original_labels, augmented_data, augmented_labels, classifier_params):
    """ä½¿ç”¨åˆ†ç±»æ€§èƒ½è¯„ä¼°å¢å¼ºè´¨é‡ - ä¿®å¤æ•°æ®æ³„éœ²é—®é¢˜"""

    print("ğŸ” è¯„ä¼°å¢å¼ºè´¨é‡...")

    # å±•å¹³æ•°æ®ç”¨äºåˆ†ç±»
    def flatten_data(data_list):
        return np.array([data.flatten() for data in data_list])

    X_orig = flatten_data(original_data)
    X_aug = flatten_data(augmented_data)

    print(f"   ğŸ“Š åŸå§‹æ•°æ®: {X_orig.shape}")
    print(f"   ğŸ“Š å¢å¼ºæ•°æ®: {X_aug.shape}")

    # è®¡ç®—å¢å¼ºæ•°æ®è´¨é‡åˆ†æ•°
    print("   ğŸ” è®¡ç®—å¢å¼ºæ•°æ®è´¨é‡åˆ†æ•°...")
    quality_scores = []
    for i, aug_sample in enumerate(augmented_data):
        # æ‰¾åˆ°å¯¹åº”çš„åŸå§‹æ ·æœ¬
        orig_sample = original_data[i % len(original_data)]
        score = compute_quality_score(aug_sample.flatten(), orig_sample.flatten())
        quality_scores.append(score)

    avg_quality_score = np.mean(quality_scores)
    print(f"   ğŸ“Š å¹³å‡è´¨é‡åˆ†æ•°: {avg_quality_score:.4f}")

    # æ•°æ®é¢„å¤„ç† - åˆ†åˆ«æ ‡å‡†åŒ–é¿å…æ•°æ®æ³„éœ²
    scaler_orig = StandardScaler()
    scaler_aug = StandardScaler()

    X_orig_scaled = scaler_orig.fit_transform(X_orig)
    X_aug_scaled = scaler_aug.fit_transform(X_aug)

    # PCAé™ç»´ (å¦‚æœç‰¹å¾å¤ªå¤š)
    if X_orig_scaled.shape[1] > 1000:
        print("   ğŸ”§ åº”ç”¨PCAé™ç»´...")
        n_components = min(100, X_orig_scaled.shape[0]-1, X_orig_scaled.shape[1])
        pca = PCA(n_components=n_components)
        X_orig_scaled = pca.fit_transform(X_orig_scaled)
        X_aug_scaled = pca.transform(X_aug_scaled)
        print(f"   ğŸ“Š é™ç»´åç‰¹å¾æ•°: {X_orig_scaled.shape[1]}")

    results = {}

    # å®éªŒ1: ä»…ä½¿ç”¨åŸå§‹æ•°æ®è®­ç»ƒå’Œæµ‹è¯• (åŸºçº¿)
    print("\n   ğŸ§ª å®éªŒ1: ä»…åŸå§‹æ•°æ® (åŸºçº¿)")
    if len(np.unique(original_labels)) > 1 and len(original_labels) >= 6:
        orig_train_X, orig_test_X, orig_train_y, orig_test_y = train_test_split(
            X_orig_scaled, original_labels, test_size=0.3, random_state=42,
            stratify=original_labels
        )

        baseline_results = train_and_evaluate_classifiers(
            orig_train_X, orig_train_y, orig_test_X, orig_test_y, classifier_params, "åŸºçº¿"
        )
        results['baseline'] = baseline_results
    else:
        print("   âš ï¸ æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡åŸºçº¿æµ‹è¯•")
        results['baseline'] = None

    # å®éªŒ2: ä½¿ç”¨åŸå§‹+å¢å¼ºæ•°æ®è®­ç»ƒï¼Œåœ¨ç‹¬ç«‹æµ‹è¯•é›†ä¸Šæµ‹è¯•
    print("\n   ğŸ§ª å®éªŒ2: åŸå§‹+å¢å¼ºæ•°æ®è®­ç»ƒï¼Œç‹¬ç«‹æµ‹è¯•é›†æµ‹è¯•")
    if results['baseline'] is not None:
        # åˆå¹¶åŸå§‹è®­ç»ƒæ•°æ®å’Œå¢å¼ºæ•°æ®è¿›è¡Œè®­ç»ƒ
        combined_train_X = np.vstack([orig_train_X, X_aug_scaled])
        combined_train_y = np.hstack([orig_train_y, augmented_labels])

        # åœ¨ç›¸åŒçš„æµ‹è¯•é›†ä¸Šæµ‹è¯•
        augmented_results = train_and_evaluate_classifiers(
            combined_train_X, combined_train_y, orig_test_X, orig_test_y, classifier_params, "å¢å¼ºè®­ç»ƒ"
        )
        results['augmented'] = augmented_results
    else:
        print("   âš ï¸ è·³è¿‡å¢å¼ºæ•°æ®æµ‹è¯•")
        results['augmented'] = None

    # å®éªŒ3: ä¸¥æ ¼çš„ç‹¬ç«‹æµ‹è¯• - å®Œå…¨åˆ†ç¦»çš„æ•°æ®é›†
    print("\n   ğŸ§ª å®éªŒ3: ä¸¥æ ¼ç‹¬ç«‹æµ‹è¯•")
    if len(original_labels) >= 10:
        # å°†åŸå§‹æ•°æ®åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šä¸€éƒ¨åˆ†ç”¨äºç”Ÿæˆå¢å¼ºæ•°æ®ï¼Œå¦ä¸€éƒ¨åˆ†ç”¨äºæµ‹è¯•
        split_idx = len(original_data) // 2

        # ç¬¬ä¸€éƒ¨åˆ†ï¼šç”¨äºè®­ç»ƒå’Œç”Ÿæˆå¢å¼ºæ•°æ®
        train_orig_data = original_data[:split_idx]
        train_orig_labels = original_labels[:split_idx]
        train_aug_data = augmented_data[:split_idx]
        train_aug_labels = augmented_labels[:split_idx]

        # ç¬¬äºŒéƒ¨åˆ†ï¼šå®Œå…¨ç‹¬ç«‹çš„æµ‹è¯•æ•°æ®
        test_orig_data = original_data[split_idx:]
        test_orig_labels = original_labels[split_idx:]

        # å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®
        train_orig_X = flatten_data(train_orig_data)
        train_aug_X = flatten_data(train_aug_data)
        test_X = flatten_data(test_orig_data)

        # æ ‡å‡†åŒ–
        scaler_strict = StandardScaler()
        train_orig_X_scaled = scaler_strict.fit_transform(train_orig_X)
        train_aug_X_scaled = scaler_strict.transform(train_aug_X)
        test_X_scaled = scaler_strict.transform(test_X)

        # PCAé™ç»´
        if train_orig_X_scaled.shape[1] > 100:
            pca_strict = PCA(n_components=min(50, train_orig_X_scaled.shape[0]-1))
            train_orig_X_scaled = pca_strict.fit_transform(train_orig_X_scaled)
            train_aug_X_scaled = pca_strict.transform(train_aug_X_scaled)
            test_X_scaled = pca_strict.transform(test_X_scaled)

        # ä»…åŸå§‹æ•°æ®è®­ç»ƒ
        strict_baseline = train_and_evaluate_classifiers(
            train_orig_X_scaled, train_orig_labels, test_X_scaled, test_orig_labels,
            classifier_params, "ä¸¥æ ¼åŸºçº¿"
        )

        # åŸå§‹+å¢å¼ºæ•°æ®è®­ç»ƒ
        combined_X = np.vstack([train_orig_X_scaled, train_aug_X_scaled])
        combined_y = np.hstack([train_orig_labels, train_aug_labels])

        strict_augmented = train_and_evaluate_classifiers(
            combined_X, combined_y, test_X_scaled, test_orig_labels,
            classifier_params, "ä¸¥æ ¼å¢å¼º"
        )

        results['strict_test'] = {
            'baseline': strict_baseline,
            'augmented': strict_augmented
        }

        print(f"   ğŸ“Š ä¸¥æ ¼æµ‹è¯• - åŸºçº¿æœ€ä½³å‡†ç¡®ç‡: {max([v['accuracy'] for v in strict_baseline.values()]):.4f}")
        print(f"   ğŸ“Š ä¸¥æ ¼æµ‹è¯• - å¢å¼ºæœ€ä½³å‡†ç¡®ç‡: {max([v['accuracy'] for v in strict_augmented.values()]):.4f}")

    else:
        print("   âš ï¸ æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡ä¸¥æ ¼æµ‹è¯•")
        results['strict_test'] = None

    # å®éªŒ4: äº¤å‰éªŒè¯è¯„ä¼°
    print("\n   ğŸ§ª å®éªŒ4: äº¤å‰éªŒè¯æ¯”è¾ƒ")
    if len(original_labels) >= 10:
        cv_results = cross_validation_comparison(
            X_orig_scaled, original_labels, X_aug_scaled, augmented_labels
        )
        results['cross_validation'] = cv_results
    else:
        print("   âš ï¸ æ•°æ®é‡ä¸è¶³ï¼Œè·³è¿‡äº¤å‰éªŒè¯")
        results['cross_validation'] = None

    # æ·»åŠ è´¨é‡åˆ†æ•°åˆ°ç»“æœ
    results['quality_score'] = avg_quality_score

    return results

def run_augmentation(data_path, output_dir, base_augmentation_params, class_intensity_factors,
                    evaluation_params, classifier_params, test_samples, seq_len):
    """å¯¹åŸå§‹æ•°æ®æ‰§è¡Œè®¤çŸ¥å¢å¼º"""

    print("ğŸ§  è®¤çŸ¥éšœç¢æ•°æ®å¢å¼ºç³»ç»Ÿ")
    print("=" * 50)

    # æ‰“å°é…ç½®ä¿¡æ¯
    print("ğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"   æ•°æ®è·¯å¾„: {data_path}")
    print(f"   è¾“å‡ºç›®å½•: {output_dir}")
    print(f"   åºåˆ—é•¿åº¦: {seq_len}")
    print(f"   å¤„ç†æ ·æœ¬æ•°: {test_samples if test_samples else 'å…¨éƒ¨'}")
    print(f"   è¯„ä¼°å‚æ•°: {evaluation_params}")
    print(f"   åˆ†ç±»å™¨æ•°é‡: {len(classifier_params)}")
    
    # 1. åŠ è½½æ•°æ®
    try:
        data_list, labels = load_data(data_path, seq_len=seq_len)
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {str(e)}")
        return

    # 2. æ•°æ®é¢„å¤„ç†
    print("ğŸ”§ æ•°æ®é¢„å¤„ç†...")
    processed_data = []
    processed_labels = []

    for i, (data, label) in enumerate(zip(data_list, labels)):
        if len(data.shape) == 1:
            data = data.reshape(-1, 1)
        elif len(data.shape) > 2:
            data = data.reshape(data.shape[0], -1)

        if np.isnan(data).any() or np.isinf(data).any():
            print(f"âš ï¸ æ ·æœ¬ {i} åŒ…å«æ— æ•ˆå€¼ï¼Œè·³è¿‡")
            continue

        processed_data.append(data)
        processed_labels.append(label)

    processed_labels = np.array(processed_labels)
    print(f"âœ… é¢„å¤„ç†å®Œæˆï¼Œä¿ç•™ {len(processed_data)} ä¸ªæœ‰æ•ˆæ ·æœ¬")
    
    # 3. åˆå§‹åŒ–å¢å¼ºç³»ç»Ÿ
    aug_system = CognitiveAugmentationSystem()

    # 4. é€‰æ‹©æ ·æœ¬è¿›è¡Œæ–¹æ³•æ¯”è¾ƒ
    sample_idx = len(processed_data) // 2
    sample_data = processed_data[sample_idx]

    print(f"ğŸ“Š ä½¿ç”¨æ ·æœ¬ {sample_idx} è¿›è¡Œæ–¹æ³•æ¯”è¾ƒ")
    print(f"ğŸ“Š æ ·æœ¬å½¢çŠ¶: {sample_data.shape}")

    # 5. åº”ç”¨å¢å¼ºæ–¹æ³•
    n_features = sample_data.shape[1]
    imu_channels = list(range(min(6, n_features)))
    pressure_channels = list(range(6, n_features)) if n_features > 6 else []

    print("ğŸ”§ åº”ç”¨å››ç§è®¤çŸ¥å¢å¼ºæ–¹æ³•...")

    augmented_results = {}
    methods = [
        ('motor_delay', imu_channels, {}),
        ('gait_perturbation', pressure_channels, {'pressure_channels': pressure_channels}),
        ('sensor_drift', imu_channels, {'imu_channels': imu_channels, 'pressure_channels': pressure_channels}),
        ('cognitive_vector', None, {})
    ]

    for method, channels, extra_params in methods:
        if method == 'motor_delay':
            params = {**base_augmentation_params[method], 'apply_to_channels': channels}
        elif method == 'gait_perturbation':
            params = {**base_augmentation_params[method], **extra_params}
        elif method == 'sensor_drift':
            params = {**base_augmentation_params[method], **extra_params}
        else:  # cognitive_vector (ç®€åŒ–ç‰ˆ)
            aug_data = sample_data.copy()
            cognitive_vector = np.random.randn(*sample_data.shape) * 0.1
            if imu_channels:
                cognitive_vector[:, imu_channels] *= 0.5
            if pressure_channels:
                cognitive_vector[:, pressure_channels] *= 0.8
            augmented_results[method] = aug_data + cognitive_vector
            continue

        aug_data = aug_system.augment_data(sample_data, method, **params)
        augmented_results[method] = aug_data

    # 6. åˆ›å»ºè¾“å‡ºç›®å½•
    os.makedirs(output_dir, exist_ok=True)

    # 7. ç”Ÿæˆå¯è§†åŒ–æ¯”è¾ƒ
    print("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–æ¯”è¾ƒ...")
    visualize_results(sample_data, augmented_results, output_dir)
    
    # 8. åŸºäºç±»åˆ«çš„æ‰¹é‡å¢å¼º
    print("ğŸ”„ æ‰§è¡ŒåŸºäºç±»åˆ«çš„æ‰¹é‡å¢å¼º...")

    # å¤„ç†æ ·æœ¬æ•°é‡
    n_process = len(processed_data) if test_samples is None else min(test_samples, len(processed_data))
    print(f"ğŸ“Š å¤„ç† {n_process} ä¸ªæ ·æœ¬")

    # æŒ‰ç±»åˆ«åˆ†ç»„æ•°æ®
    class_data = {0: [], 1: [], 2: []}
    class_indices = {0: [], 1: [], 2: []}

    for i, (data, label) in enumerate(zip(processed_data[:n_process], processed_labels[:n_process])):
        if label in class_data:
            class_data[label].append(data)
            class_indices[label].append(i)

    print(f"ğŸ“Š ç±»åˆ«åˆ†å¸ƒ: {[len(class_data[i]) for i in range(3)]}")

    # åº”ç”¨ç±»åˆ«ç‰¹å®šçš„å¢å¼º
    all_aug_data = []
    all_aug_labels = []

    for class_label in [0, 1, 2]:
        if len(class_data[class_label]) == 0:
            continue

        print(f"ğŸ”§ å¢å¼ºç±»åˆ« {class_label} ({len(class_data[class_label])} ä¸ªæ ·æœ¬)...")

        # è·å–ç±»åˆ«ç‰¹å®šå‚æ•°
        class_params = get_class_specific_params(class_label, base_augmentation_params, class_intensity_factors)

        # ä½¿ç”¨ç±»åˆ«ç‰¹å®šå‚æ•°åº”ç”¨è¿åŠ¨å»¶è¿Ÿå¢å¼º
        motor_params = class_params['motor_delay']
        class_aug_data, class_aug_labels = aug_system.batch_augment(
            class_data[class_label], [class_label] * len(class_data[class_label]),
            'motor_delay', **motor_params, apply_to_channels=imu_channels
        )

        all_aug_data.extend(class_aug_data)
        all_aug_labels.extend(class_aug_labels)

    print(f"ğŸ“Š æ€»å¢å¼ºæ ·æœ¬æ•°: {len(all_aug_data)}")
    
    # 9. è¯„ä¼°å¢å¼ºè´¨é‡
    print("ğŸ” è¯„ä¼°å¢å¼ºè´¨é‡...")

    # å‡†å¤‡è¯„ä¼°æ•°æ®
    original_data = processed_data[:n_process]
    original_labels = processed_labels[:n_process]

    # æå–ä»…å¢å¼ºçš„æ ·æœ¬ï¼ˆæ’é™¤å¢å¼ºæ•°æ®ä¸­çš„åŸå§‹æ ·æœ¬ï¼‰
    augmented_only_data = []
    augmented_only_labels = []

    for i in range(0, len(all_aug_data), 2):  # è·³è¿‡åŸå§‹æ ·æœ¬ï¼Œåªå–å¢å¼ºæ ·æœ¬
        if i + 1 < len(all_aug_data):
            augmented_only_data.append(all_aug_data[i + 1])
            augmented_only_labels.append(all_aug_labels[i + 1])

    # è¯„ä¼°è´¨é‡
    quality_metrics = evaluate_augmentation_quality(
        original_data, original_labels,
        augmented_only_data, augmented_only_labels,
        classifier_params
    )

    # æ‰“å°è¯„ä¼°ç»“æœ
    print("\n" + "="*60)
    print("ğŸ“Š å¢å¼ºè´¨é‡è¯„ä¼°ç»“æœ")
    print("="*60)

    # è·å–æœ€ä½³åˆ†ç±»å™¨ç»“æœï¼ˆéšæœºæ£®æ—ï¼‰
    rf_baseline = quality_metrics['baseline']['Random Forest']
    rf_augmented = quality_metrics['augmented']['Random Forest']

    print(f"ğŸ“Š åŸºç¡€æµ‹è¯• - åŸå§‹æ•°æ®å‡†ç¡®ç‡: {rf_baseline['accuracy']:.4f}")
    print(f"ğŸ“Š åŸºç¡€æµ‹è¯• - å¢å¼ºæ•°æ®å‡†ç¡®ç‡: {rf_augmented['accuracy']:.4f}")
    improvement = rf_augmented['accuracy'] - rf_baseline['accuracy']
    relative_improvement = improvement / rf_baseline['accuracy'] * 100 if rf_baseline['accuracy'] > 0 else 0
    print(f"ğŸ“ˆ åŸºç¡€æµ‹è¯•æ€§èƒ½æå‡: {improvement:.4f} ({relative_improvement:.2f}%)")

    # ä¸¥æ ¼ç‹¬ç«‹æµ‹è¯•ç»“æœ
    if quality_metrics['strict_test'] is not None:
        strict_baseline = quality_metrics['strict_test']['baseline']
        strict_augmented = quality_metrics['strict_test']['augmented']

        strict_base_best = max([v['accuracy'] for v in strict_baseline.values()])
        strict_aug_best = max([v['accuracy'] for v in strict_augmented.values()])
        strict_improvement = strict_aug_best - strict_base_best

        print(f"\nğŸ“Š ä¸¥æ ¼ç‹¬ç«‹æµ‹è¯• - åŸºçº¿æœ€ä½³å‡†ç¡®ç‡: {strict_base_best:.4f}")
        print(f"ğŸ“Š ä¸¥æ ¼ç‹¬ç«‹æµ‹è¯• - å¢å¼ºæœ€ä½³å‡†ç¡®ç‡: {strict_aug_best:.4f}")
        print(f"ğŸ“ˆ ä¸¥æ ¼æµ‹è¯•æ€§èƒ½æå‡: {strict_improvement:.4f}")

        if strict_improvement > 0:
            print("âœ… å¢å¼ºæ•°æ®åœ¨ä¸¥æ ¼æµ‹è¯•ä¸­æ˜¾ç¤ºæ­£å‘æ•ˆæœ")
        elif strict_improvement == 0:
            print("âš ï¸ å¢å¼ºæ•°æ®åœ¨ä¸¥æ ¼æµ‹è¯•ä¸­æ— æ˜æ˜¾æ•ˆæœ")
        else:
            print("âŒ å¢å¼ºæ•°æ®åœ¨ä¸¥æ ¼æµ‹è¯•ä¸­æ˜¾ç¤ºè´Ÿå‘æ•ˆæœ")

    # äº¤å‰éªŒè¯ç»“æœ
    if quality_metrics['cross_validation'] is not None:
        cv_results = quality_metrics['cross_validation']
        print(f"\nğŸ“Š äº¤å‰éªŒè¯æå‡: {cv_results['improvement']:.4f}")

    # è´¨é‡åˆ†æ•°
    print(f"\nğŸ“Š å¢å¼ºæ•°æ®è´¨é‡åˆ†æ•°: {quality_metrics['quality_score']:.4f}")

    # å„åˆ†ç±»å™¨æ€§èƒ½å¯¹æ¯”
    print("\nğŸ“Š å„åˆ†ç±»å™¨æ€§èƒ½å¯¹æ¯” (åŸºç¡€æµ‹è¯•):")
    for clf_name in ['Random Forest', 'Gradient Boosting', 'SVM', 'MLP']:
        if clf_name in quality_metrics['baseline']:
            baseline_acc = quality_metrics['baseline'][clf_name]['accuracy']
            augmented_acc = quality_metrics['augmented'][clf_name]['accuracy']
            improvement = augmented_acc - baseline_acc
            print(f"   {clf_name}: {baseline_acc:.3f} -> {augmented_acc:.3f} ({improvement:+.3f})")

    # 10. ä¿å­˜ç»“æœ
    print("\nğŸ’¾ ä¿å­˜å¢å¼ºç»“æœ...")

    # ä¿å­˜ä¸ºNumPyæ ¼å¼
    original_array = np.array([data.flatten() for data in original_data])
    augmented_array = np.array([data.flatten() for data in all_aug_data])

    np.save(os.path.join(output_dir, 'original.npy'), original_array)
    np.save(os.path.join(output_dir, 'augmented.npy'), augmented_array)
    np.save(os.path.join(output_dir, 'labels.npy'), np.array(all_aug_labels))

    # ä¿å­˜ä¸ºCSVæ ¼å¼
    pd.DataFrame(original_array).assign(label=original_labels).to_csv(
        os.path.join(output_dir, 'original.csv'), index=False)
    pd.DataFrame(augmented_array).assign(label=all_aug_labels).to_csv(
        os.path.join(output_dir, 'augmented.csv'), index=False)

    # ä¿å­˜è´¨é‡è¯„ä¼°æŒ‡æ ‡
    import json
    with open(os.path.join(output_dir, 'quality_metrics.json'), 'w', encoding='utf-8') as f:
        # è½¬æ¢numpyç±»å‹ä¸ºPythonç±»å‹ä»¥ä¾¿JSONåºåˆ—åŒ–
        metrics_for_json = {
            'basic_test': {
                'baseline_accuracy': float(rf_baseline['accuracy']),
                'augmented_accuracy': float(rf_augmented['accuracy']),
                'improvement': float(improvement),
                'relative_improvement': float(relative_improvement)
            },
            'quality_score': float(quality_metrics['quality_score']),
            'classifiers_performance': {
                clf_name: {
                    'baseline': float(quality_metrics['baseline'][clf_name]['accuracy']),
                    'augmented': float(quality_metrics['augmented'][clf_name]['accuracy']),
                    'improvement': float(quality_metrics['augmented'][clf_name]['accuracy'] -
                                       quality_metrics['baseline'][clf_name]['accuracy'])
                }
                for clf_name in ['Random Forest', 'Gradient Boosting', 'SVM', 'MLP']
                if clf_name in quality_metrics['baseline']
            }
        }

        # æ·»åŠ ä¸¥æ ¼æµ‹è¯•ç»“æœ
        if quality_metrics['strict_test'] is not None:
            strict_baseline = quality_metrics['strict_test']['baseline']
            strict_augmented = quality_metrics['strict_test']['augmented']
            strict_base_best = max([v['accuracy'] for v in strict_baseline.values()])
            strict_aug_best = max([v['accuracy'] for v in strict_augmented.values()])

            metrics_for_json['strict_test'] = {
                'baseline_best_accuracy': float(strict_base_best),
                'augmented_best_accuracy': float(strict_aug_best),
                'improvement': float(strict_aug_best - strict_base_best)
            }

        # æ·»åŠ äº¤å‰éªŒè¯ç»“æœ
        if quality_metrics['cross_validation'] is not None:
            cv_results = quality_metrics['cross_validation']
            metrics_for_json['cross_validation'] = {
                'improvement': float(cv_results['improvement'])
            }

        json.dump(metrics_for_json, f, indent=2, ensure_ascii=False)

    print(f"âœ… å¢å¼ºå®Œæˆ! ç»“æœä¿å­˜åœ¨: {output_dir}")
    print(f"ğŸ“Š åŸå§‹: {len(original_data)} -> å¢å¼º: {len(all_aug_data)} (å¢é•¿ {len(all_aug_data)/len(original_data):.1f}x)")
    print(f"ğŸ“ˆ è´¨é‡æå‡: {relative_improvement:.2f}%")

def main():
    """ä¸»å‡½æ•° - é›†ä¸­æ‰€æœ‰å¯é…ç½®å‚æ•°"""
    print("ğŸ§  è®¤çŸ¥éšœç¢æ•°æ®å¢å¼ºç³»ç»Ÿ")
    print("=" * 50)

    # ==================== æ ¸å¿ƒå‚æ•°è®¾ç½® ====================
    # æ•°æ®è·¯å¾„è®¾ç½®
    data_path = "./original_processed"  # åŸå§‹æ•°æ®ç›®å½•
    output_dir = "augmentation_results"  # è¾“å‡ºç›®å½•

    # æ•°æ®å¤„ç†å‚æ•°
    seq_len = 400  # åºåˆ—é•¿åº¦
    test_samples = None  # å¤„ç†æ ·æœ¬æ•°ï¼ŒNone=å…¨éƒ¨

    # åŸºç¡€å¢å¼ºå‚æ•°
    base_augmentation_params = {
        'motor_delay': {'delay_range': [3, 8], 'alpha_range': [0.75, 0.9]},
        'gait_perturbation': {'perturbation_intensity': 0.25, 'sync_probability': 0.3},
        'sensor_drift': {'drift_intensity': 0.08}
    }

    # ç±»åˆ«ç‰¹å®šå¢å¼ºå¼ºåº¦è°ƒèŠ‚å› å­
    class_intensity_factors = {
        0: {'delay_factor': 0.5, 'perturbation_factor': 0.4, 'drift_factor': 0.25},  # æ­£å¸¸äººï¼šæœ€å°å¢å¼º
        1: {'delay_factor': 0.75, 'perturbation_factor': 0.8, 'drift_factor': 0.625},  # è½»åº¦è®¤çŸ¥éšœç¢ï¼šä¸­ç­‰å¢å¼º
        2: {'delay_factor': 1.25, 'perturbation_factor': 1.6, 'drift_factor': 1.5}   # ä¸­åº¦è®¤çŸ¥è¡°é€€ï¼šå¼ºçƒˆå¢å¼º
    }

    # è¯„ä¼°å‚æ•°
    evaluation_params = {
        'test_size': 0.3,  # æµ‹è¯•é›†æ¯”ä¾‹
        'cv_folds': 5,     # äº¤å‰éªŒè¯æŠ˜æ•°
        'random_state': 42, # éšæœºç§å­
        'pca_components': 100,  # PCAé™ç»´åçš„ç‰¹å¾æ•°
        'quality_threshold': 0.6  # è´¨é‡åˆ†æ•°é˜ˆå€¼
    }

    # åˆ†ç±»å™¨å‚æ•°
    classifier_params = {
        'random_forest': {
            'n_estimators': 200,
            'max_depth': 10,
            'min_samples_split': 5,
            'min_samples_leaf': 2
        },
        'gradient_boosting': {
            'n_estimators': 100,
            'learning_rate': 0.1,
            'max_depth': 6
        },
        'svm': {
            'C': 1.0,
            'gamma': 'scale',
            'kernel': 'rbf'
        },
        'mlp': {
            'hidden_layer_sizes': (128, 64, 32),
            'activation': 'relu',
            'solver': 'adam',
            'alpha': 0.001,
            'learning_rate': 'adaptive',
            'max_iter': 1000
        }
    }
    # ================================================

    if not os.path.exists(data_path):
        print(f"âŒ é”™è¯¯: ç›®å½•ä¸å­˜åœ¨: {data_path}")
        print("ğŸ’¡ è¯·ç¡®ä¿originalç›®å½•å­˜åœ¨å¹¶åŒ…å«CSVæ–‡ä»¶")
        return

    try:
        run_augmentation(
            data_path=data_path,
            output_dir=output_dir,
            base_augmentation_params=base_augmentation_params,
            class_intensity_factors=class_intensity_factors,
            evaluation_params=evaluation_params,
            classifier_params=classifier_params,
            test_samples=test_samples,
            seq_len=seq_len
        )
    except Exception as e:
        print(f"âŒ é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
